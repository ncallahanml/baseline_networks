{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3e9272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "from wave_generator import WaveGen\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from ray import train, tune\n",
    "# from ray.train import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309db4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def torch_train_test_split(*tensors, split=.8):\n",
    "    n_samples = tensors[0].shape[0]\n",
    "    train_size = int(split * n_samples)\n",
    "    test_size = n_samples - train_size\n",
    "    rand_indices = torch.randperm(n_samples)\n",
    "    train_indices, test_indices = torch.split(rand_indices, [train_size, test_size])\n",
    "    assert train_indices.shape[0] == train_size, f'{train_indices.shape} != {train_size}'\n",
    "    assert test_indices.shape[0] == n_samples - train_size, f'{test_indices.shape} != {n_samples - train_size}'\n",
    "\n",
    "    data_tensors = list()\n",
    "    for tensor in tensors:\n",
    "        assert tensor.shape[0] == n_samples\n",
    "        train_data = tensor[train_indices]\n",
    "        test_data = tensor[test_indices]\n",
    "        data_tensors.extend((train_data, test_data))\n",
    "    return data_tensors\n",
    "    \n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, seq2seq_tensor, enc_window=120, dec_window=120):\n",
    "        assert seq2seq_tensor.ndim == 2\n",
    "        assert seq2seq_tensor.shape[1] > enc_window + dec_window, f'{seq2seq_tensor.shape} | {enc_window} | {dec_window}'\n",
    "        self._data = seq2seq_tensor\n",
    "        self.enc_window = enc_window\n",
    "        self.dec_window = dec_window\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self._data.shape[1] - (self.enc_window + self.dec_window), 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        enc_data = self._data[:,index:index + self.enc_window]\n",
    "        dec_data = self._data[:,index + self.enc_window:self.enc_window + self.dec_window + index]\n",
    "        return enc_data, dec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466936f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_load_data_fn(\n",
    "    size=2048,\n",
    "    slope_min=.0001,\n",
    "    slope_max=.001,\n",
    "    n_channels=64,\n",
    "    n_periods=45,\n",
    "    enc_window = 128,\n",
    "    dec_window = 128,\n",
    "    plot=True,\n",
    "):\n",
    "    arr_dict = dict()\n",
    "    slopes = np.linspace(slope_min, slope_max, n_channels)\n",
    "\n",
    "    for slope in slopes:\n",
    "        arr = WaveGen(size=size).linear_phase(n_periods=n_periods).cos().amp(2).t_noise(std=.01, dof=2).sample(n_samples=1).samples\n",
    "        trend = np.exp(np.cumsum(np.log(np.random.normal(slope, .005, size=size) + 1))) - 1\n",
    "        arr_dict[slope] = np.squeeze(arr + trend)\n",
    "\n",
    "    if plot:\n",
    "        for slope in np.random.choice(slopes, size=3):\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.plot(arr_dict[slope].squeeze())\n",
    "            plt.show()\n",
    "    \n",
    "    data = torch.from_numpy(np.stack(list(arr_dict.values()), axis=1))\n",
    "    print(data.shape)\n",
    "    split_idx = int(data.shape[0]*.8)\n",
    "    train_data = data[:split_idx].transpose(0, 1)\n",
    "    test_data = data[split_idx:].transpose(0, 1)\n",
    "\n",
    "    def load_data():\n",
    "        train_dataset = TorchDataset(train_data, enc_window=enc_window, dec_window=dec_window)\n",
    "        test_dataset = TorchDataset(test_data, enc_window=enc_window, dec_window=dec_window)\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    return load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65ce24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SineCNNLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size,\n",
    "        output_size,\n",
    "        in_channels=64,\n",
    "        out_channels=128,\n",
    "        kernel_size=2, \n",
    "        stride=1, \n",
    "        drop=.4, \n",
    "        hidden_size=32,\n",
    "        num_layers=3,\n",
    "        **extra,\n",
    "    ):\n",
    "        super(SineCNNLSTM, self).__init__()\n",
    "        \n",
    "#         pool_out_shape = (conv1_out_shape - pool_kernel) // pool_kernel + 1\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        conv1_out_shape = (input_size - kernel_size) // stride + 1\n",
    "        conv2_out_shape = (conv1_out_shape - kernel_size) // stride + 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(conv2_out_shape, hidden_size, num_layers=num_layers, dropout=drop)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.output = nn.Sigmoid()\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a42075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SineCNNLSTM2(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        enc_window,\n",
    "        dec_window,\n",
    "        hidden_dim=64,\n",
    "        output_dim=128,\n",
    "        n_lstm_layers=3,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        drop=.4,\n",
    "    ):\n",
    "        super(SineCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.enc_window = enc_window\n",
    "        self.dec_window = dec_window\n",
    "        \n",
    "        self.conv_in = self.single_conv(1, hidden_dim)\n",
    "        self.conv_hidden = self.single_conv(hidden_dim, hidden_dim)\n",
    "        self.conv_out = self.single_conv(hidden_dim, output_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, num_layers=n_lstm_layers, dropout=drop)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        return\n",
    "        \n",
    "    def single_conv(self, in_channels, out_channels):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride,\n",
    "            ),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    def conv_forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_hidden(x)\n",
    "        x = self.conv_out(x)\n",
    "        print(x.shape)\n",
    "        x = self.gap(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.conv_forward(x[i:i+self.enc_window]) \n",
    "            for i \n",
    "            in range(x.shape[0] - self.enc_window)\n",
    "        ])\n",
    "        print('pre-lstm', x.shape)\n",
    "        x = self.lstm(x)\n",
    "        x = self.activation(x)\n",
    "        print('post-lstm', x.shape)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        label = label[i+self.enc_window:i+self.enc_window+self.dec_window]\n",
    "        loss = self.criterion(pred, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8891a41d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "def train_lstm(\n",
    "    config,\n",
    "    load_data_fn,\n",
    "    load_model_fn,\n",
    "    val_split=.8,\n",
    "):\n",
    "    model = load_model_fn(config)\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    \n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state['epoch']\n",
    "        net.load_state_dict(checkpoint_state['net_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint_state['optimizer_state_dict'])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    \n",
    "    train_dataset, test_dataset = load_data_fn()\n",
    "    k = int(len(train_dataset) * val_split)\n",
    "    train_dataset, val_dataset = train_dataset[:k], train_dataset[k:]\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    for epoch in range(start_epoch, 10):\n",
    "        running_train_loss = 0.\n",
    "        running_test_loss = 0.\n",
    "        for (train_data, train_labels), (test_data, test_labels) in zip(train_dataloader, test_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(train_data[i])\n",
    "            train_loss = criterion(train_output, train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += train_loss.item()\n",
    "\n",
    "            for i in range(test_data.shape[0]):\n",
    "                with torch.no_grad():\n",
    "                    test_output = model(test_data[i])\n",
    "                    test_loss = criterion(test_output, test_labels[i])\n",
    "                    running_test_loss += test_loss.item()\n",
    "                \n",
    "            checkpoint_data = {\n",
    "                'epoch' : epoch,\n",
    "#                 'net_state_dict' : model.state_dict(),\n",
    "                'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            }\n",
    "            train.report(checkpoint_data)\n",
    "        if epoch == 9:\n",
    "            torch.save(model.state_dict(), './model.pth')\n",
    "#             checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "            \n",
    "#             session.report(\n",
    "#                 {'loss' : test_loss / test_data.shape[0]},\n",
    "#                 checkpoint=checkpoint,\n",
    "#             )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d117679",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-12-05 21:40:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:59.93        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.9/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 64.000: None | Iter 32.000: None | Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None<br>Logical resource usage: 3.0/0 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 17<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lstm_1d2f6_00000</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00000_0_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00001</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00001_1_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00002</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00002_2_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00003</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00003_3_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00004</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00004_4_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00005</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00005_5_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00006</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00006_6_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00007</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00007_7_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00008</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00008_8_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00009</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00009_9_2023-12-05_21-39-52\\error.txt </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00010</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00010_10_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00011</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00011_11_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00012</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00012_12_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00013</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00013_13_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00014</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00014_14_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00015</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00015_15_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00016</td><td style=\"text-align: right;\">           1</td><td>C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00016_16_2023-12-05_21-39-52\\error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lstm_1d2f6_00017</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00018</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00019</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00001</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00002</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00003</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00004</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00005</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00006</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00007</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00008</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00009</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00010</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00011</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00012</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00013</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00014</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00015</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>train_lstm_1d2f6_00016</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,352 E 32244 38368] (raylet.exe) agent_manager.cc:70: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent/15724.\n",
      "\u001b[33m(raylet)\u001b[0m The raylet fate shares with the agent. This can happen because\n",
      "\u001b[33m(raylet)\u001b[0m - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n",
      "\u001b[33m(raylet)\u001b[0m - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.\n",
      "\u001b[33m(raylet)\u001b[0m - The agent is killed by the OS (e.g., out of memory).\n",
      "\u001b[33m(raylet)\u001b[0m *** SIGTERM received at time=1701833992 ***\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FF6276885C6  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FF62769EE86  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FF62769E5BE  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FFCEB481BB2  (unknown)  configthreadlocale\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FFCED367344  (unknown)  BaseThreadInitThunk\n",
      "\u001b[33m(raylet)\u001b[0m     @   00007FFCED9E26B1  (unknown)  RtlUserThreadStart\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361: *** SIGTERM received at time=1701833992 ***\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361:     @   00007FF6276885C6  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361:     @   00007FF62769EE86  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361:     @   00007FF62769E5BE  (unknown)  (unknown)\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361:     @   00007FFCEB481BB2  (unknown)  configthreadlocale\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361:     @   00007FFCED367344  (unknown)  BaseThreadInitThunk\n",
      "\u001b[33m(raylet)\u001b[0m [2023-12-05 21:39:52,360 E 32244 38368] (raylet.exe) logging.cc:361:     @   00007FFCED9E26B1  (unknown)  RtlUserThreadStart\n",
      "2023-12-05 21:40:06,462\tWARNING worker.py:2074 -- The node with node id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb and address: 127.0.0.1 and node name: 127.0.0.1 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a \t(1) raylet crashes unexpectedly (OOM, preempted node, etc.) \n",
      "\t(2) raylet has lagging heartbeats due to slow network or busy workload.\n",
      "2023-12-05 21:40:06,468\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00016\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: b2e5721381301accbf10185201000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 127.0.0.1 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,518\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00014\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: e96b745429ce01cdbc0ea73c01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,536\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: fae5b13867f4d3fda335eef301000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,551\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00012\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: ca85de8e11b024dc49f9b87b01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,557\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 592dd6c5594df72b1e28f87901000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:40:06,561\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00005\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 47b941adcba9388a1354406c01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,564\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 309f38909cfbd36b28bd38e601000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,568\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00004\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: bebd23e04b6cf838a650b49801000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,572\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00011\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 0dfc928da93a0048250436ad01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,575\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00013\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: a1a047284d5c7e53df03019701000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,578\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00009\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: ac0062369b642fce8d5074e001000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,581\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00015\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 73e5cd1421c1d773b1be1a0401000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:40:06,584\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 7b85a25c6cc74ee586acd17d01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,588\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00007\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: 6912e95ac98636cdfbbba15e01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,591\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00010\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: ddc2baf1f2da407a45f835be01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,594\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00006\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: cd8b50c36bdff30e22eb9c9301000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:06,598\tERROR tune_controller.py:1383 -- Trial task failed for trial train_lstm_1d2f6_00008\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\air\\execution\\_internal\\event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\_private\\worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: wrap_function.<locals>.ImplicitFunc\n",
      "\tactor_id: ab957b0c3d5d39a390bb7f4a01000000\n",
      "\tnamespace: feb35eb7-9213-43e2-8ede-0849be26c112\n",
      "The actor is dead because its node has died. Node Id: 6474594995bd499cc3a9b3024d41feecefc8e1c01ca5bd0f66fd8adb\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "2023-12-05 21:40:12,755\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:13,268\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:13,779\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:14,292\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:14,804\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:14,913\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:15,423\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:15,924\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:16,427\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:16,938\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:20,010\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:20,520\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:21,031\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:21,545\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:22,057\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:25,010\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:25,519\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:26,033\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:40:26,550\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:27,052\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:30,024\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:30,540\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:31,053\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:31,568\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:32,082\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:35,039\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:35,549\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:36,060\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:36,572\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:37,085\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:40,046\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:40,560\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:41,064\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:41,578\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:42,090\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:45,054\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:45,569\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:46,084\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:46,586\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:47,097\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n",
      "2023-12-05 21:40:50,074\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #2...\n",
      "2023-12-05 21:40:50,587\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #3...\n",
      "2023-12-05 21:40:51,092\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #4...\n",
      "2023-12-05 21:40:51,593\tWARNING resource_updater.py:262 -- Cluster resources not detected or are 0. Attempt #5...\n",
      "2023-12-05 21:40:52,107\tWARNING resource_updater.py:275 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.\n"
     ]
    }
   ],
   "source": [
    "def hyperparameter_tune(total_size=2048, input_size=128, output_size=128, in_channels=16, cpus=4, num_samples=10, max_t=100, grace_period=2, reduction_factor=2):\n",
    "    model_load_fn = lambda x : SineCNNLSTM(input_size, output_size, in_channels=in_channels, **x)\n",
    "    \n",
    "    config = {\n",
    "        'out_channels' : [16, 32, 64],\n",
    "        'kernel_size' : [2,8,24,48],\n",
    "        'stride' : [1,2,4,8,24],\n",
    "        'drop' : [.2, .4, .6],\n",
    "        'hidden_size' : [16, 32, 64],\n",
    "        'num_layers' : [4,8],\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric='loss',\n",
    "        mode='min',\n",
    "        max_t=max_t,\n",
    "        grace_period=grace_period,\n",
    "        reduction_factor=reduction_factor,\n",
    "    )\n",
    "\n",
    "    load_data_fn = get_load_data_fn(total_size, n_channels=in_channels, enc_window=input_size, dec_window=output_size)\n",
    "    tuner = tune.Tuner(\n",
    "        partial(train_lstm, load_data_fn, model_load_fn, val_split=.8),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=20,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    result = tuner.fit()\n",
    "    \n",
    "    best_trial = result.get_best_result('loss', mode='min')\n",
    "    print(best_trial)\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final test loss: {best_trial.last_result['loss']}\")\n",
    "    \n",
    "    best_model = model_load_fn(\n",
    "        out_channels=best_trial.config['out_channels'], \n",
    "        kernel_size=best_trial.config['kernel_size'], \n",
    "        stride=best_trial.config['stride'],\n",
    "        drop=best_trial.config['drop'],\n",
    "        hidden_size=best_trial.config['hidden_size'],\n",
    "        num_layers=best_trial.config['num_layers'],\n",
    "    )\n",
    "#     best_checkpoint_data = best_trial.checkpoint.to_air_checkpoint().to_dict()\n",
    "\n",
    "    state_dict = torch.load(os.path.join(best_model.path, 'model.pth'))\n",
    "    best_model.load_state_dict(state_dict)\n",
    "    return best_model\n",
    "\n",
    "best_model = hyperparameter_tune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

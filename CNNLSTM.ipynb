{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3e9272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "from wave_generator import WaveGen\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from ray import train, tune\n",
    "# from ray.train import Checkpoint, session\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309db4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def torch_train_test_split(*tensors, split=.8):\n",
    "    n_samples = tensors[0].shape[0]\n",
    "    train_size = int(split * n_samples)\n",
    "    test_size = n_samples - train_size\n",
    "    rand_indices = torch.randperm(n_samples)\n",
    "    train_indices, test_indices = torch.split(rand_indices, [train_size, test_size])\n",
    "    assert train_indices.shape[0] == train_size, f'{train_indices.shape} != {train_size}'\n",
    "    assert test_indices.shape[0] == n_samples - train_size, f'{test_indices.shape} != {n_samples - train_size}'\n",
    "\n",
    "    data_tensors = list()\n",
    "    for tensor in tensors:\n",
    "        assert tensor.shape[0] == n_samples\n",
    "        train_data = tensor[train_indices]\n",
    "        test_data = tensor[test_indices]\n",
    "        data_tensors.extend((train_data, test_data))\n",
    "    return data_tensors\n",
    "    \n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, seq2seq_tensor, enc_window=120, dec_window=120):\n",
    "        assert seq2seq_tensor.ndim == 2\n",
    "        assert seq2seq_tensor.shape[1] > enc_window + dec_window, f'{seq2seq_tensor.shape} | {enc_window} | {dec_window}'\n",
    "        self._data = seq2seq_tensor\n",
    "        self.enc_window = enc_window\n",
    "        self.dec_window = dec_window\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self._data.shape[1] - (self.enc_window + self.dec_window), 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.shape[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        enc_data = self._data[:,index:index + self.enc_window]\n",
    "        dec_data = self._data[:,index + self.enc_window:self.enc_window + self.dec_window + index]\n",
    "        return enc_data, dec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466936f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_load_data_fn(\n",
    "    size=2048,\n",
    "    slope_min=.0001,\n",
    "    slope_max=.001,\n",
    "    n_channels=64,\n",
    "    n_periods=45,\n",
    "    enc_window = 128,\n",
    "    dec_window = 128,\n",
    "    plot=True,\n",
    "):\n",
    "    arr_dict = dict()\n",
    "    slopes = np.linspace(slope_min, slope_max, n_channels)\n",
    "\n",
    "    for slope in slopes:\n",
    "        arr = WaveGen(size=size).linear_phase(n_periods=n_periods).cos().amp(2).t_noise(std=.01, dof=2).sample(n_samples=1).samples\n",
    "        trend = np.exp(np.cumsum(np.log(np.random.normal(slope, .005, size=size) + 1))) - 1\n",
    "        arr_dict[slope] = np.squeeze(arr + trend)\n",
    "\n",
    "    if plot:\n",
    "        for slope in np.random.choice(slopes, size=3):\n",
    "            plt.figure(figsize=(15,5))\n",
    "            plt.plot(arr_dict[slope].squeeze())\n",
    "            plt.show()\n",
    "    \n",
    "    data = torch.from_numpy(np.stack(list(arr_dict.values()), axis=1))\n",
    "    print(data.shape)\n",
    "    split_idx = int(data.shape[0]*.8)\n",
    "    train_data = data[:split_idx].transpose(0, 1)\n",
    "    test_data = data[split_idx:].transpose(0, 1)\n",
    "\n",
    "    def load_data():\n",
    "        train_dataset = TorchDataset(train_data, enc_window=enc_window, dec_window=dec_window)\n",
    "        test_dataset = TorchDataset(test_data, enc_window=enc_window, dec_window=dec_window)\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    return load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ce24b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SineCNNLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size,\n",
    "        output_size,\n",
    "        in_channels=64,\n",
    "        out_channels=128,\n",
    "        kernel_size=2, \n",
    "        stride=1, \n",
    "        drop=.4, \n",
    "        hidden_size=32,\n",
    "        num_layers=3,\n",
    "        **extra,\n",
    "    ):\n",
    "        super(SineCNNLSTM, self).__init__()\n",
    "        \n",
    "#         pool_out_shape = (conv1_out_shape - pool_kernel) // pool_kernel + 1\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        conv1_out_shape = (input_size - kernel_size) // stride + 1\n",
    "        conv2_out_shape = (conv1_out_shape - kernel_size) // stride + 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(conv2_out_shape, hidden_size, num_layers=num_layers, dropout=drop)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.output = nn.Sigmoid()\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a42075",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SineCNNLSTM2(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        enc_window,\n",
    "        dec_window,\n",
    "        hidden_dim=64,\n",
    "        output_dim=128,\n",
    "        n_lstm_layers=3,\n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        drop=.4,\n",
    "    ):\n",
    "        super(SineCNNLSTM, self).__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.enc_window = enc_window\n",
    "        self.dec_window = dec_window\n",
    "        \n",
    "        self.conv_in = self.single_conv(1, hidden_dim)\n",
    "        self.conv_hidden = self.single_conv(hidden_dim, hidden_dim)\n",
    "        self.conv_out = self.single_conv(hidden_dim, output_dim)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(output_dim, hidden_dim, num_layers=n_lstm_layers, dropout=drop)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        return\n",
    "        \n",
    "    def single_conv(self, in_channels, out_channels):\n",
    "        layer = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                kernel_size=self.kernel_size,\n",
    "                stride=self.stride,\n",
    "            ),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    def conv_forward(self, x):\n",
    "        x = self.conv_in(x)\n",
    "        x = self.conv_hidden(x)\n",
    "        x = self.conv_out(x)\n",
    "        print(x.shape)\n",
    "        x = self.gap(x)\n",
    "        print(x.shape)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.cat([\n",
    "            self.conv_forward(x[i:i+self.enc_window]) \n",
    "            for i \n",
    "            in range(x.shape[0] - self.enc_window)\n",
    "        ])\n",
    "        print('pre-lstm', x.shape)\n",
    "        x = self.lstm(x)\n",
    "        x = self.activation(x)\n",
    "        print('post-lstm', x.shape)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        label = label[i+self.enc_window:i+self.enc_window+self.dec_window]\n",
    "        loss = self.criterion(pred, label)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891a41d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "def train_lstm(\n",
    "    config,\n",
    "    load_data_fn,\n",
    "    load_model_fn,\n",
    "    val_split=.8,\n",
    "):\n",
    "    model = load_model_fn(config)\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=config['lr'])\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    \n",
    "    if checkpoint:\n",
    "        checkpoint_state = checkpoint.to_dict()\n",
    "        start_epoch = checkpoint_state['epoch']\n",
    "        net.load_state_dict(checkpoint_state['net_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint_state['optimizer_state_dict'])\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "    \n",
    "    train_dataset, test_dataset = load_data_fn()\n",
    "    k = int(len(train_dataset) * val_split)\n",
    "    train_dataset, val_dataset = train_dataset[:k], train_dataset[k:]\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    for epoch in range(start_epoch, 10):\n",
    "        running_train_loss = 0.\n",
    "        running_test_loss = 0.\n",
    "        for (train_data, train_labels), (test_data, test_labels) in zip(train_dataloader, test_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(train_data[i])\n",
    "            train_loss = criterion(train_output, train_labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_train_loss += train_loss.item()\n",
    "\n",
    "            for i in range(test_data.shape[0]):\n",
    "                with torch.no_grad():\n",
    "                    test_output = model(test_data[i])\n",
    "                    test_loss = criterion(test_output, test_labels[i])\n",
    "                    running_test_loss += test_loss.item()\n",
    "                \n",
    "            checkpoint_data = {\n",
    "                'epoch' : epoch,\n",
    "#                 'net_state_dict' : model.state_dict(),\n",
    "                'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            }\n",
    "            train.report(checkpoint_data)\n",
    "        if epoch == 9:\n",
    "            torch.save(model.state_dict(), './model.pth')\n",
    "#             checkpoint = Checkpoint.from_dict(checkpoint_data)\n",
    "            \n",
    "#             session.report(\n",
    "#                 {'loss' : test_loss / test_data.shape[0]},\n",
    "#                 checkpoint=checkpoint,\n",
    "#             )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d117679",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 21:41:31,812\tERROR tune.py:1043 -- Trials did not complete: [train_lstm_1d2f6_00000, train_lstm_1d2f6_00001, train_lstm_1d2f6_00002, train_lstm_1d2f6_00003, train_lstm_1d2f6_00004, train_lstm_1d2f6_00005, train_lstm_1d2f6_00006, train_lstm_1d2f6_00007, train_lstm_1d2f6_00008, train_lstm_1d2f6_00009, train_lstm_1d2f6_00010, train_lstm_1d2f6_00011, train_lstm_1d2f6_00012, train_lstm_1d2f6_00013, train_lstm_1d2f6_00014, train_lstm_1d2f6_00015, train_lstm_1d2f6_00016]\n",
      "2023-12-05 21:41:31,812\tINFO tune.py:1047 -- Total run time: 99.69 seconds (87.49 seconds for the tuning loop).\n",
      "2023-12-05 21:41:31,813\tWARNING tune.py:1062 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44\", trainable=...)\n",
      "2023-12-05 21:41:31,822\tWARNING experiment_analysis.py:185 -- Failed to fetch metrics for 20 trial(s):\n",
      "- train_lstm_1d2f6_00000: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00000: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00000_0_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00001: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00001: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00001_1_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00002: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00002: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00002_2_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00003: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00003: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00003_3_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00004: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00004: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00004_4_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00005: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00005: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00005_5_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00006: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00006: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00006_6_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00007: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00007: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00007_7_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00008: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00008: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00008_8_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00009: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00009: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00009_9_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00010: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00010: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00010_10_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00011: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00011: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00011_11_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00012: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00012: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00012_12_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00013: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00013: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00013_13_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00014: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00014: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00014_14_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00015: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00015: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00015_15_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00016: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00016: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00016_16_2023-12-05_21-39-52')\n",
      "- train_lstm_1d2f6_00017: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00017: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00017_17_2023-12-05_21-40-06')\n",
      "- train_lstm_1d2f6_00018: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00018: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00018_18_2023-12-05_21-40-06')\n",
      "- train_lstm_1d2f6_00019: FileNotFoundError('Could not fetch metrics for train_lstm_1d2f6_00019: both result.json and progress.csv were not found at C:/Users/Nick/ray_results/train_lstm_2023-12-05_21-39-44/train_lstm_1d2f6_00019_19_2023-12-05_21-40-06')\n",
      "2023-12-05 21:41:31,905\tWARNING experiment_analysis.py:575 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     best_model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model\n\u001b[1;32m---> 51\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparameter_tune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m, in \u001b[0;36mhyperparameter_tune\u001b[1;34m(total_size, input_size, output_size, in_channels, cpus, num_samples, max_t, grace_period, reduction_factor)\u001b[0m\n\u001b[0;32m     22\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[0;32m     23\u001b[0m     partial(train_lstm, load_data_fn, model_load_fn, val_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.8\u001b[39m),\n\u001b[0;32m     24\u001b[0m     tune_config\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mTuneConfig(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m result \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m---> 32\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_trial)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_trial\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ray\\tune\\result_grid.py:162\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[1;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[0;32m    151\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo best trial found for the given metric: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experiment_analysis\u001b[38;5;241m.\u001b[39mdefault_metric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means that no trial has reported this metric\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[0;32m    156\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filter_nan_and_inf\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No best trial found for the given metric: loss. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "def hyperparameter_tune(total_size=2048, input_size=128, output_size=128, in_channels=16, cpus=4, num_samples=10, max_t=100, grace_period=2, reduction_factor=2):\n",
    "    model_load_fn = lambda x : SineCNNLSTM(input_size, output_size, in_channels=in_channels, **x)\n",
    "    \n",
    "    config = {\n",
    "        'out_channels' : [16, 32, 64],\n",
    "        'kernel_size' : [2,8,24,48],\n",
    "        'stride' : [1,2,4,8,24],\n",
    "        'drop' : [.2, .4, .6],\n",
    "        'hidden_size' : [16, 32, 64],\n",
    "        'num_layers' : [4,8],\n",
    "    }\n",
    "\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric='loss',\n",
    "        mode='min',\n",
    "        max_t=max_t,\n",
    "        grace_period=grace_period,\n",
    "        reduction_factor=reduction_factor,\n",
    "    )\n",
    "\n",
    "    load_data_fn = get_load_data_fn(total_size, n_channels=in_channels, enc_window=input_size, dec_window=output_size)\n",
    "    tuner = tune.Tuner(\n",
    "        partial(train_lstm, load_data_fn, model_load_fn, val_split=.8),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=20,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    result = tuner.fit()\n",
    "    \n",
    "    best_trial = result.get_best_result('loss', mode='min')\n",
    "    print(best_trial)\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final test loss: {best_trial.last_result['loss']}\")\n",
    "    \n",
    "    best_model = model_load_fn(\n",
    "        out_channels=best_trial.config['out_channels'], \n",
    "        kernel_size=best_trial.config['kernel_size'], \n",
    "        stride=best_trial.config['stride'],\n",
    "        drop=best_trial.config['drop'],\n",
    "        hidden_size=best_trial.config['hidden_size'],\n",
    "        num_layers=best_trial.config['num_layers'],\n",
    "    )\n",
    "#     best_checkpoint_data = best_trial.checkpoint.to_air_checkpoint().to_dict()\n",
    "\n",
    "    state_dict = torch.load(os.path.join(best_model.path, 'model.pth'))\n",
    "    best_model.load_state_dict(state_dict)\n",
    "    return best_model\n",
    "\n",
    "best_model = hyperparameter_tune()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "r = 1024\n",
    "x = torch.linspace(0, n*2*np.pi, 256*2)\n",
    "x2 = x.unsqueeze(1).repeat_interleave(r, dim=1)\n",
    "y = torch.sin(x2 + torch.normal(0., np.pi, size=(1,x2.shape[1])))\n",
    "y += torch.normal(0., 0.2, size=(r,))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(data=y[:,[1,2,3,4]].numpy(), dashes=False)\n",
    "plt.legend([], [])\n",
    "plt.show()\n",
    "\n",
    "y_grad = torch.tensor(np.gradient(y, axis=0)).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_schedule(t_max=4000, s=0.008, max_beta=0.999):\n",
    "    # https://github.com/ageron/handson-ml3/blob/main/17_autoencoders_gans_and_diffusion_models.ipynb\n",
    "    t_tensor = torch.arange(t_max)\n",
    "    f = torch.cos((t_tensor / t_max + s) / (1 + s) * np.pi / 2) ** 2\n",
    "    alpha = torch.clamp(f[1:] / f[:-1], 1 - max_beta, 1)\n",
    "    alpha = torch.cat([torch.tensor(1).unsqueeze(dim=0), alpha]).float()\n",
    "    beta = 1 - alpha\n",
    "    alpha_cumprod = torch.cumprod(alpha, 0)\n",
    "    return alpha, alpha_cumprod, beta  # αₜ , α̅ₜ , βₜ for t = 0 to T\n",
    "\n",
    "alpha, alpha_cumprod, beta = variance_schedule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a5038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_out_size(conv_in, kernel_size, stride=1):\n",
    "    # assumes no padding\n",
    "    return math.floor((conv_in - (kernel_size - 1) - 1) / stride + 1)\n",
    "\n",
    "def single_conv(in_channels, out_channels, kernel_size=3, pad=True):\n",
    "    layer = nn.Sequential(\n",
    "        nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            padding=0 if not pad else math.floor(kernel_size / 2),\n",
    "        ),\n",
    "        nn.BatchNorm1d(out_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "def double_conv(in_channels, out_channels, **conv_kwargs):\n",
    "    layer = nn.Sequential(\n",
    "        single_conv(in_channels, out_channels, **conv_kwargs),\n",
    "        single_conv(out_channels, out_channels, **conv_kwargs),\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "def triple_conv(in_channels, mid_channels, out_channels, **conv_kwargs):\n",
    "    layer = nn.Sequential(\n",
    "        single_conv(in_channels, mid_channels, **conv_kwargs),\n",
    "        single_conv(mid_channels, mid_channels, **conv_kwargs),\n",
    "        single_conv(mid_channels, out_channels, **conv_kwargs),\n",
    "    )\n",
    "    return layer\n",
    "\n",
    "class Unet1d(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        init_upscale=8,\n",
    "        n_blocks=3,\n",
    "        depth_factor=3,\n",
    "        kernel_size=3,\n",
    "        stride=2,\n",
    "        in_channels=1,\n",
    "    ):\n",
    "        super(Unet1d, self).__init__()\n",
    "        self.conv_down_blocks = list()\n",
    "        self.pool_down_layers = list()\n",
    "        self.conv_up_blocks = list()\n",
    "        self.tran_up_layers = list()\n",
    "        assert kernel_size % 2\n",
    "\n",
    "        self.input = single_conv(in_channels, init_upscale)\n",
    "        channels = init_upscale\n",
    "        for i in range(n_blocks):\n",
    "            self.conv_down_blocks.append(\n",
    "                double_conv(\n",
    "                    channels, \n",
    "                    channels * depth_factor,\n",
    "                )\n",
    "            )\n",
    "            channels *= depth_factor\n",
    "            self.pool_down_layers.append(\n",
    "                nn.MaxPool1d(stride)\n",
    "            )\n",
    "            \n",
    "        self.mid_layer = triple_conv(channels, channels * depth_factor, channels)\n",
    "        for i in range(n_blocks):\n",
    "            self.tran_up_layers.append(\n",
    "                nn.ConvTranspose1d(\n",
    "                    channels, \n",
    "                    channels, \n",
    "                    kernel_size, \n",
    "                    stride,\n",
    "                    padding=math.floor(kernel_size / 2),\n",
    "                    output_padding=1,\n",
    "                )\n",
    "            )\n",
    "            self.conv_up_blocks.append(\n",
    "                triple_conv(\n",
    "                    channels, # * 2\n",
    "                    channels, \n",
    "                    channels // depth_factor,\n",
    "                )\n",
    "            )\n",
    "            channels //= depth_factor\n",
    "            \n",
    "        self.output = single_conv(channels, 1)\n",
    "        return\n",
    "           \n",
    "    def forward(self, x):\n",
    "        original_shape = x.shape\n",
    "        residuals = list()\n",
    "        x = self.input(x)\n",
    "        for conv_down_block, pool_down_layer in zip(self.conv_down_blocks, self.pool_down_layers):\n",
    "            x = conv_down_block(x)\n",
    "            residuals.append(x)\n",
    "            x = pool_down_layer(x)\n",
    "            \n",
    "        x = self.mid_layer(x)\n",
    "        for conv_up_block, residual, tran_up_layer in zip(self.conv_up_blocks, reversed(residuals), self.tran_up_layers):\n",
    "            x = tran_up_layer(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = conv_up_block(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        assert x.shape == original_shape, f'{x.shape} != {original_shape}'\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1f427",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "n_epochs = 380\n",
    "\n",
    "model = Unet1d(n_blocks=5, kernel_size=17)\n",
    "\n",
    "optimizer = optim.NAdam(model.parameters(), lr=.1)\n",
    "# annealer = lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs, eta_min=1e-5)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "norm_data = (data - data.mean()) / data.std()\n",
    "model_loss = list()\n",
    "for epoch in range(n_epochs):\n",
    "    if not epoch % 10:\n",
    "        print(epoch, end='\\r')\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(norm_data)\n",
    "        \n",
    "    loss = criterion(preds, noise)\n",
    "    loss.backward()\n",
    "    \n",
    "    model_loss.append(loss.item())\n",
    "    optimizer.step()\n",
    "#     annealer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7cb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

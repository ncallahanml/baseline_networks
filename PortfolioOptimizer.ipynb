{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user \"jax[cpu]\"\n",
    "# %pip install --user flax\n",
    "# %pip install --user optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import flax\n",
    "import jax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "# not a fan of the FLAX/JAX nn imports, especially considering most models in this repository use PyTorch nn\n",
    "import flax.linen as ln\n",
    "import jax.numpy as jnp\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from flax.training import train_state, checkpoints\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Callable, Any, Tuple\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ce14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(ln.Module):\n",
    "    initializer : Callable[[Any, Tuple[int], Any], Any]\n",
    "    \n",
    "    @ln.compact\n",
    "    def __call__(self, x):\n",
    "        x = ln.Dense(features=1, use_bias=False, kernel_init=self.initializer)(x)\n",
    "        return x\n",
    "    \n",
    "# https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.ipynb#scrollTo=3qjS60Zl-I2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5626a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianReturns(Dataset):\n",
    "    def __init__(self, size, cov, mean, cov_noise=None, noise_batch=32, reshuffle=True, seed=None):\n",
    "        super().__init__()\n",
    "        cov = np.asarray(cov)\n",
    "        mean = np.asarray(mean)\n",
    "        self._data = self._generate_returns(size, mean, cov, cov_noise, noise_batch, reshuffle)\n",
    "        return\n",
    "        \n",
    "    def _noisy_cov(self, cov, cov_noise):\n",
    "        return np.clip(cov + np.random.normal(cov.shape) * cov_noise, 0, 1)\n",
    "        \n",
    "    def _generate_returns(self, size, mean, cov, cov_noise, noise_batch, reshuffle):\n",
    "        n_even = size // noise_batch\n",
    "        size_uneven = size % noise_batch\n",
    "        samples = list()\n",
    "        assert cov.shape[0] == cov.shape[1] == mean.shape[0], f'{cov.shape} | {mean.shape}'\n",
    "        \n",
    "        for _ in range(n_even):\n",
    "            # should provide option to input list of cov_noise offsets\n",
    "            ncov = cov if cov_noise is None else self._noisy_cov(cov, cov_noise)\n",
    "            sample = st.multivariate_normal(mean, cov).rvs(size=noise_batch)\n",
    "            assert sample.shape == (noise_batch, cov.shape[0])\n",
    "            samples.append(sample)\n",
    "            \n",
    "        if size_uneven:\n",
    "            key1, key2 = jax.random.split(self.rng, num=2)\n",
    "            ncov = cov if cov_noise is None else self._noisy_cov(cov, cov_noise)\n",
    "            sample = st.multivariate_normal(mean, cov).rvs(size=size_uneven)\n",
    "            samples.append(sample)\n",
    "            \n",
    "        return_array = np.concatenate(samples, axis=0)\n",
    "        assert return_array.shape == (size, cov.shape[1])\n",
    "        if reshuffle:\n",
    "            return_array = np.random.permutation(return_array)\n",
    "        return return_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._data.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._data.shape\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self._data[idx]\n",
    "        return data_point\n",
    "    \n",
    "# This collate function is taken from the JAX tutorial with PyTorch Data Loading\n",
    "# https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "cov = [[.01, .001], [.001, .03]] # covariance matrix\n",
    "bias = [.001, .002] # expected return\n",
    "\n",
    "dataset = GaussianReturns(256, cov, bias)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is pretty awful design but should otherwise be split into separate functions\n",
    "def initialize_params(params, in_place=True, method='even', weight_layer='Dense_0', **reference_weights):\n",
    "    # methods in even, exact, average, proportional\n",
    "    w = params['params'][weight_layer]['kernel']\n",
    "    assert w.ndim == 2 and w.shape[1] == 1\n",
    "    if method == 'even':\n",
    "        # no reference required\n",
    "        n_assets = w.shape[0]\n",
    "        weights = jnp.full_like(w, 1/n_assets)\n",
    "    elif method == 'exact':\n",
    "        # requires 'exact_weights'\n",
    "        if 'exact_weights' not in reference_weights: \n",
    "            raise KeyError('Must pass desired weights as `exact_weights` keyword argument')\n",
    "        weights = reference_weights['exact']\n",
    "    elif method == 'average':\n",
    "        if 'min_weights' not in reference_weights:\n",
    "            raise KeyError('Must pass desired min weights as `min_weights` keyword argument')\n",
    "        if 'max_weights' not in reference_weights:\n",
    "            raise KeyError('Must pass desired max weights as `max_weights` keyword argument')\n",
    "        min_w = jnp.asarray(reference_weights['min_weights'])\n",
    "        max_w = jnp.asarray(reference_weights['max_weights'])\n",
    "        weights = min_w + max_w / 2\n",
    "    elif method == 'proportional':\n",
    "        if 'totals' not in reference_weights:\n",
    "            raise KeyError('Must pass desired array for proportionality as `totals` keyword argument')\n",
    "        total_w = jnp.asarray(reference_weights['total'])\n",
    "        total_sum = total_w.sum()\n",
    "        weights = total_w / total_sum\n",
    "    \n",
    "    weights = jnp.expand_dims(weights, axis=1) if weights.ndim == 1 else weights\n",
    "    assert w.shape == weights.shape, f'{w.shape} != {weights.shape}'\n",
    "    assert weights.sum() == 1\n",
    "    if not in_place:\n",
    "        params = deepcopy(params)\n",
    "        \n",
    "    params['params'][weight_layer]['kernel'] = weights\n",
    "    return params\n",
    "\n",
    "def even_init(key, size, dtype, **kwargs):\n",
    "    assert len(size) == 1 or size[1] == 1, f'Invalid shape {size}'\n",
    "    even_w = 1 / size[0]\n",
    "#     w = jnp.full(size, even_w, dtype=dtype)\n",
    "    init = ln.initializers.constant(even_w)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "rng, inpt_rng, init_rng = jax.random.split(rng, 3)\n",
    "\n",
    "inpt_array = jax.random.normal(inpt_rng, (8, 2))\n",
    "\n",
    "model = Linear(ln.initializers.glorot_normal())\n",
    "params = jax.jit(model.init)(init_rng, inpt_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26014bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb7490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(\n",
    "    state, \n",
    "    params, \n",
    "    data, \n",
    "    constraints=[(.6, .8),(.2, .4)], \n",
    "    gamma=[.5, .2, .2, 1.2], \n",
    "    min_var=.00001, \n",
    "    min_cvar=-.05\n",
    "):\n",
    "    k = int(data.shape[0] * .05)\n",
    "    min_weights, max_weights = zip(*constraints)\n",
    "    pfl_returns = state.apply_fn(params, data).squeeze()\n",
    "    exp_mean = pfl_returns.mean(axis=0)\n",
    "    exp_var = pfl_returns.std(axis=0)\n",
    "    exp_cvar = jax.lax.slice_in_dim(jnp.argpartition(pfl_returns, k), 0, k, axis=0).mean()\n",
    "\n",
    "    w = params['params']['Dense_0']['kernel']\n",
    "    \n",
    "    # sharpe, minimized instead of maximized\n",
    "    loss = -exp_mean / (jnp.minimum(exp_var, min_var))\n",
    "    loss += gamma[0] * (jnp.sum(w) - 1) ** 2\n",
    "    loss += gamma[1] * (jnp.minimum(w - jnp.array(min_weights), 0).sum() ** 2)\n",
    "    loss += gamma[2] * (jnp.minimum(jnp.array(max_weights) - w, 0).sum() ** 2)\n",
    "    loss += gamma[3] * (jnp.minimum(exp_cvar - min_cvar, 0).sum() ** 2)\n",
    "    return loss\n",
    "\n",
    "def cvar_loss(state, params, data):\n",
    "    k = int(data.shape[0] * .05)\n",
    "    pfl_returns = state.apply_fn(params, data).squeeze()\n",
    "    exp_mean = jnp.mean(pfl_returns)\n",
    "    exp_cvar = jax.lax.slice(jnp.argpartition(pfl_returns, k), (0,), (k,)).mean()\n",
    "    \n",
    "    loss = exp_mean / (exp_mean - exp_cvar) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d604bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        sharpe_loss,  # Function to calculate the loss\n",
    "        argnums=1,  # Parameters are second argument of the function\n",
    "        has_aux=False,\n",
    "    )\n",
    "    loss, grads = grad_fn(state, state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(state, dataloader, num_epochs=100):\n",
    "    losss = list()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_losss = list()\n",
    "        for batch in dataloader:\n",
    "            state, loss = train_step(state, batch)\n",
    "            epoch_losss.append(loss)\n",
    "        \n",
    "        losss.append(jnp.mean(jnp.array(epoch_losss)))\n",
    "                   \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(losss)\n",
    "    plt.show()\n",
    "    return state\n",
    "\n",
    "def single_batch_train_model(state, data, num_epochs=100):\n",
    "    weights = list()\n",
    "    losss = list()\n",
    "#     data = jnp.expand_dims(data, axis=0)\n",
    "    print(data.shape)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        state, loss = train_step(state, data)\n",
    "#         display(state)\n",
    "#         assert False\n",
    "        weights.append(state.params['params']['Dense_0']['kernel'].ravel())\n",
    "        losss.append(loss)  \n",
    "    \n",
    "    print(weights[-1])\n",
    "    df = pd.DataFrame({f'A{i}' : weight_arr[10:] for i, weight_arr in enumerate(zip(*weights))})\n",
    "    display(df)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    sns.lineplot(data=df.astype(np.float32))\n",
    "    plt.show()\n",
    "            \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(losss)\n",
    "    plt.show()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_state = single_batch_train_model(\n",
    "    model_state, \n",
    "    dataset._data, \n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints.save_checkpoint(\n",
    "    ckpt_dir='my_checkpoints/',\n",
    "    target=trained_model_state,\n",
    "    step=100,\n",
    "    prefix='my_model',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19729517",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_state = checkpoints.restore_checkpoint(\n",
    "    ckpt_dir='my_checkpoints/',\n",
    "    target=model_state,\n",
    "    prefix='my_model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.bind(trained_model_state.params)\n",
    "output = trained_model(dataset._data)\n",
    "display(dir(trained_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

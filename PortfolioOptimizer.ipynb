{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1404e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user \"jax[cpu]\"\n",
    "# %pip install --user flax\n",
    "# %pip install --user optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4027a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import flax\n",
    "import jax\n",
    "\n",
    "import scipy.stats as st\n",
    "# not a fan of the FLAX nn import, especially considering most models in this repository us PyTorch nn\n",
    "import flax.linen as ln\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "683ce14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(ln.Module):\n",
    "    num_outputs : int\n",
    "        \n",
    "    @ln.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = ln.Dense(features=self.num_outputs, use_bias=False)(x)\n",
    "        x = ln.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.ipynb#scrollTo=3qjS60Zl-I2_\n",
    "model = Linear(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5626a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianReturns(Dataset):\n",
    "    def __init__(self, size, cov, mean, cov_noise=None, noise_batch=32, reshuffle=True, seed=None):\n",
    "        super().__init__()\n",
    "        cov = np.asarray(cov)\n",
    "        mean = np.asarray(mean)\n",
    "        self._data = self._generate_returns(size, mean, cov, cov_noise, noise_batch, reshuffle)\n",
    "        return\n",
    "        \n",
    "    def _noisy_cov(self, cov, cov_noise):\n",
    "        return np.clip(cov + np.random.normal(cov.shape) * cov_noise, 0, 1)\n",
    "        \n",
    "    def _generate_returns(self, size, mean, cov, cov_noise, noise_batch, reshuffle):\n",
    "        n_even = size // noise_batch\n",
    "        size_uneven = size % noise_batch\n",
    "        samples = list()\n",
    "        assert cov.shape[0] == cov.shape[1] == mean.shape[0], f'{cov.shape} | {mean.shape}'\n",
    "        \n",
    "        for _ in range(n_even):\n",
    "            # should provide option to input list of cov_noise offsets\n",
    "            ncov = cov if cov_noise is None else self._noisy_cov(cov, cov_noise)\n",
    "            sample = st.multivariate_normal(mean, cov).rvs(size=noise_batch)\n",
    "            assert sample.shape == (noise_batch, cov.shape[0])\n",
    "            samples.append(sample)\n",
    "            \n",
    "        if size_uneven:\n",
    "            key1, key2 = jax.random.split(self.rng, num=2)\n",
    "            ncov = cov if cov_noise is None else self._noisy_cov(cov, cov_noise)\n",
    "            sample = st.multivariate_normal(mean, cov).rvs(size=size_uneven)\n",
    "            samples.append(sample)\n",
    "            \n",
    "        return_array = np.concatenate(samples, axis=0)\n",
    "        assert return_array.shape == (size, cov.shape[1])\n",
    "        if reshuffle:\n",
    "            return_array = np.random.permutation(return_array)\n",
    "        return return_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._data.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._data.shape\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self._data[idx]\n",
    "        return data_point\n",
    "    \n",
    "# This collate function is taken from the JAX tutorial with PyTorch Data Loading\n",
    "# https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "dataset = GaussianReturns(256, [[.001, .0001], [.0001, .001]], [.001, .002])\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26014bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "rng, inpt_rng, init_rng = jax.random.split(rng, 3)\n",
    "\n",
    "inpt_array = jax.random.normal(inpt_rng, (8, 2))\n",
    "params = model.init(init_rng, inpt_array)\n",
    "optimizer = optax.sgd(learning_rate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb7490b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8a481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_returns(state, params, data):\n",
    "    logits = state.apply_fn(params, data)\n",
    "    #####################################\n",
    "    logits = jnp.clip(logits, .2, .7)\n",
    "    #####################################\n",
    "    weights = ln.softmax(logits)\n",
    "    pfl_returns = jnp.multiply(data, weights).sum(axis=1)    \n",
    "    return pfl_returns\n",
    "    \n",
    "def sharpe_loss(state, params, data):\n",
    "    pfl_returns = dot_returns(state, params, data)\n",
    "#     exp_return = jnp.exp(jnp.sum(jnp.log(pfl_returns + 1))) -1\n",
    "    exp_mean = pfl_returns.mean(axis=0)\n",
    "    exp_var = pfl_returns.var(axis=0)\n",
    "    \n",
    "    # sharpe, minimized instead of maximized\n",
    "    loss = -exp_mean / exp_var   \n",
    "    return loss\n",
    "\n",
    "def cvar_loss(state, params, data):\n",
    "    k = int(data.shape[0] * .05)\n",
    "    pfl_returns = dot_returns(state, params, data)\n",
    "#     exp_return = jnp.exp(jnp.sum(jnp.log(pfl_returns + 1))) -1\n",
    "    exp_mean = jnp.mean(pfl_returns)\n",
    "    exp_cvar = jax.lax.slice(jnp.argpartition(pfl_returns, k), (0,), (k,)).mean()\n",
    "    \n",
    "    # sharpe, minimized instead of maximized\n",
    "    loss = exp_mean / (exp_mean - exp_cvar)   \n",
    "    return loss\n",
    "\n",
    "def cvar_sharpe_loss(state, params, data, min_cvar=-.1, penalty=100):\n",
    "    k = int(data.shape[0] * .05)\n",
    "    pfl_returns = dot_returns(state, params, data)\n",
    "    exp_mean = jnp.mean(pfl_returns)\n",
    "    exp_var = pfl_returns.var(axis=0)\n",
    "    exp_cvar = jax.lax.slice_in_dim(jnp.argpartition(pfl_returns, k), 0, k, axis=0)\n",
    "    \n",
    "    # big loss increase if exp_cvar exceeds min_cvar\n",
    "    loss = jnp.mean(-exp_mean / exp_var + penalty * jnp.amin(jnp.array([0, exp_cvar - min_cvar])))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "176f142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(data_loader))\n",
    "# calculate_loss_acc(model_state, model_state.params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d604bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        cvar_sharpe_loss,  # Function to calculate the loss\n",
    "        argnums=1,  # Parameters are second argument of the function\n",
    "        has_aux=False,\n",
    "    )\n",
    "    loss, grads = grad_fn(state, state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec3014d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1117e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(state, dataloader, num_epochs=100):\n",
    "    losss = list()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_losss = list()\n",
    "        for batch in dataloader:\n",
    "            state, loss = train_step(state, batch)\n",
    "            epoch_losss.append(loss)\n",
    "        \n",
    "        losss.append(jnp.mean(jnp.array(epoch_losss)))\n",
    "                   \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(losss)\n",
    "    plt.show()\n",
    "    return state\n",
    "\n",
    "def single_batch_train_model(state, data, num_epochs=100):\n",
    "    weights = list()\n",
    "    losss = list()\n",
    "#     data = jnp.expand_dims(data, axis=0)\n",
    "    print(data.shape)\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        state, loss = train_step(state, data)\n",
    "#         display(state)\n",
    "#         assert False\n",
    "        weights.append(state.params['params']['Dense_0']['kernel'].ravel())\n",
    "        losss.append(loss)  \n",
    "    \n",
    "    print(weights[-1])\n",
    "    df = pd.DataFrame({f'A{i}' : weight_arr[10:] for i, weight_arr in enumerate(zip(*weights))})\n",
    "    display(df)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    sns.lineplot(data=df.astype(np.float32))\n",
    "    plt.show()\n",
    "            \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(losss)\n",
    "    plt.show()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93fdbdbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataset._data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "857d23ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (2, 2) but got shape (512, 2) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_model_state \u001b[38;5;241m=\u001b[39m \u001b[43msingle_batch_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m, in \u001b[0;36msingle_batch_train_model\u001b[1;34m(state, data, num_epochs)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[1;32m---> 25\u001b[0m         state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#         display(state)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#         assert False\u001b[39;00m\n\u001b[0;32m     28\u001b[0m         weights\u001b[38;5;241m.\u001b[39mappend(state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mravel())\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(state, batch)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(state, batch):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Gradient function\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     grad_fn \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(\n\u001b[0;32m      5\u001b[0m         cvar_sharpe_loss,  \u001b[38;5;66;03m# Function to calculate the loss\u001b[39;00m\n\u001b[0;32m      6\u001b[0m         argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Parameters are second argument of the function\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m     )\n\u001b[1;32m----> 9\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, loss\n",
      "    \u001b[1;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[7], line 33\u001b[0m, in \u001b[0;36mcvar_sharpe_loss\u001b[1;34m(state, params, data, min_cvar, penalty)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcvar_sharpe_loss\u001b[39m(state, params, data, min_cvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m.1\u001b[39m, penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     32\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m.05\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m     pfl_returns \u001b[38;5;241m=\u001b[39m \u001b[43mdot_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     exp_mean \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mmean(pfl_returns)\n\u001b[0;32m     35\u001b[0m     exp_var \u001b[38;5;241m=\u001b[39m pfl_returns\u001b[38;5;241m.\u001b[39mvar(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36mdot_returns\u001b[1;34m(state, params, data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot_returns\u001b[39m(state, params, data):\n\u001b[1;32m----> 2\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#####################################\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mclip(logits, \u001b[38;5;241m.2\u001b[39m, \u001b[38;5;241m.7\u001b[39m)\n",
      "    \u001b[1;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36mLinear.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;129m@ln\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      6\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mln\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     x \u001b[38;5;241m=\u001b[39m ln\u001b[38;5;241m.\u001b[39msigmoid(x)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flax\\linen\\linear.py:235\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    227\u001b[0m   \u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    242\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[0;32m    244\u001b[0m     )\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flax\\core\\scope.py:971\u001b[0m, in \u001b[0;36mScope.param\u001b[1;34m(self, name, init_fn, unbox, *init_args)\u001b[0m\n\u001b[0;32m    966\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[0;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[1;32m--> 971\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[0;32m    972\u001b[0m           name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[0;32m    973\u001b[0m       )\n\u001b[0;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    975\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (2, 2) but got shape (512, 2) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "trained_model_state = single_batch_train_model(\n",
    "    model_state, \n",
    "    jnp.expand_dims(dataset._data, axis=0), \n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_state = train_model(\n",
    "    model_state, \n",
    "    dataloader, \n",
    "    num_epochs=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2220c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints.save_checkpoint(\n",
    "    ckpt_dir='my_checkpoints/',\n",
    "    target=trained_model_state,\n",
    "    step=100,\n",
    "    prefix='my_model',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19729517",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_state = checkpoints.restore_checkpoint(\n",
    "    ckpt_dir='my_checkpoints/',\n",
    "    target=model_state,\n",
    "    prefix='my_model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.bind(trained_model_state.params)\n",
    "output = trained_model(dataset._data)\n",
    "display(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

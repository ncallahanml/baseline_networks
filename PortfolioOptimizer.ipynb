{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4f3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --user \"jax[cpu]\"\n",
    "# %pip install --user flax\n",
    "# %pip install --user optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6832232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flax\n",
    "import optax\n",
    "\n",
    "# not a fan of the FLAX nn import, especially considering most models in this repository us PyTorch nn\n",
    "import flax.linen as ln\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "728a5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(ln.Module):\n",
    "    num_outputs : int\n",
    "        \n",
    "    @ln.compact\n",
    "    def __call__(self, x):\n",
    "        x = ln.Dense(features=self.num_outputs)(x)\n",
    "        x = ln.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "# https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.ipynb#scrollTo=3qjS60Zl-I2_\n",
    "model = Linear(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "class GaussianReturns(Dataset):\n",
    "    def __init__(self, size, cov, mean, cov_noise=None, noise_batch=32, reshuffle=True, seed=None):\n",
    "        super().__init__()\n",
    "        cov = np.asarray(cov)\n",
    "        mean = np.asarray(mean)\n",
    "        self._data = self._generate_returns(size, mean, cov, cov_noise, noise_batch, reshuffle)\n",
    "        return\n",
    "        \n",
    "    def _noisy_cov(self, cov, cov_noise):\n",
    "        return np.clip(cov + np.random.normal(cov.shape) * cov_noise, 0, 1)\n",
    "        \n",
    "    def _generate_returns(self, size, mean, cov, cov_noise, noise_batch, reshuffle):\n",
    "        n_even = size // noise_batch\n",
    "        size_uneven = size % noise_batch\n",
    "        samples = list()\n",
    "        assert cov.shape[0] == cov.shape[1] == mean.shape[0], f'{cov.shape} | {mean.shape}'\n",
    "        \n",
    "        for _ in range(n_even):\n",
    "            # should provide option to input list of cov_noise offsets\n",
    "            ncov = cov if cov_noise is None else self._noisy_cov(cov, cov_noise)\n",
    "            sample = st.multivariate_normal(mean, cov).rvs(size=noise_batch)\n",
    "            assert sample.shape == (noise_batch, cov.shape[0])\n",
    "            samples.append(sample)\n",
    "            \n",
    "        if size_uneven:\n",
    "            key1, key2 = jax.random.split(self.rng, num=2)\n",
    "            ncov = cov if cov_noise is None else self._noisy_cov(cov, cov_noise)\n",
    "            sample = st.multivariate_normal(mean, cov).rvs(size=size_uneven)\n",
    "            samples.append(sample)\n",
    "            \n",
    "        return_array = np.concatenate(samples, axis=0)\n",
    "        assert return_array.shape == (size, cov.shape[1])\n",
    "        if reshuffle:\n",
    "            return_array = np.random.permutation(return_array)\n",
    "        return return_array\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._data.shape[0]\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._data.shape\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self._data[idx]\n",
    "        return data_point\n",
    "    \n",
    "\n",
    "# This collate function is taken from the JAX tutorial with PyTorch Data Loading\n",
    "# https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "dataset = GaussianReturns(256, [[.001, .0001], [.0001, .001]], [.001, .002])\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79036a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "rng, inpt_rng, init_rng = jax.random.split(rng, 3)\n",
    "\n",
    "# should be changed to He initialization?\n",
    "inpt_array = jax.random.normal(inpt_rng, (8, 2))\n",
    "params = model.init(init_rng, inpt_array)\n",
    "optimizer = optax.sgd(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee461587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "model_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=params,\n",
    "    tx=optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4627b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_returns(state, params, data):\n",
    "    logits = state.apply_fn(params, data)\n",
    "    weights = ln.softmax(logits)\n",
    "    pfl_returns = jnp.multiply(data, weights).sum(axis=1)    \n",
    "    return pfl_returns\n",
    "    \n",
    "def sharpe_loss(state, params, data):\n",
    "    pfl_returns = dot_returns(state, params, data)\n",
    "#     exp_return = jnp.exp(jnp.sum(jnp.log(pfl_returns + 1))) -1\n",
    "    exp_mean = pfl_returns.mean(axis=0)\n",
    "    exp_var = pfl_returns.var(axis=0)\n",
    "    \n",
    "    # sharpe, minimized instead of maximized\n",
    "    loss = -exp_mean / exp_var   \n",
    "    return loss\n",
    "\n",
    "def cvar_loss(state, params, data):\n",
    "    k = int(data.shape[0] * .05)\n",
    "    pfl_returns = dot_returns(state, params, data)\n",
    "#     exp_return = jnp.exp(jnp.sum(jnp.log(pfl_returns + 1))) -1\n",
    "    exp_mean = jnp.mean(pfl_returns)\n",
    "    exp_cvar = jax.lax.slice(jnp.argpartition(pfl_returns, k), (0,), (k,)).mean()\n",
    "    \n",
    "    # sharpe, minimized instead of maximized\n",
    "    loss = exp_mean / (exp_mean - exp_cvar)   \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1731b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(data_loader))\n",
    "# calculate_loss_acc(model_state, model_state.params, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91118e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def train_step(state, batch):\n",
    "    # Gradient function\n",
    "    grad_fn = jax.value_and_grad(\n",
    "        sharpe_loss,  # Function to calculate the loss\n",
    "        argnums=1,  # Parameters are second argument of the function\n",
    "        has_aux=False,\n",
    "    )\n",
    "    loss, grads = grad_fn(state, state.params, batch)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34f61f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit  # Jit the function for efficiency\n",
    "def eval_step(state, batch):\n",
    "    # Determine the accuracy\n",
    "    _, acc = calculate_loss_acc(state, state.params, batch)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a83dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(state, dataloader, num_epochs=100):\n",
    "    losss = list()\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_losss = list()\n",
    "        for batch in dataloader:\n",
    "            state, loss = train_step(state, batch)\n",
    "            epoch_losss.append(loss)\n",
    "        losss.append(jnp.mean(jnp.array(epoch_losss)))\n",
    "            \n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(losss)\n",
    "    plt.show()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3263b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 71494/100000 [01:21<00:32, 885.97it/s]"
     ]
    }
   ],
   "source": [
    "trained_model_state = train_model(model_state, dataloader, num_epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba31f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9881eee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoints' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcheckpoints\u001b[49m\u001b[38;5;241m.\u001b[39msave_checkpoint(\n\u001b[0;32m      2\u001b[0m     ckpt_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_checkpoints/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m     target\u001b[38;5;241m=\u001b[39mtrained_model_state,\n\u001b[0;32m      4\u001b[0m     step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      5\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmy_model\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkpoints' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoints.save_checkpoint(\n",
    "    ckpt_dir='my_checkpoints/',\n",
    "    target=trained_model_state,\n",
    "    step=100,\n",
    "    prefix='my_model',\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8726a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_state = checkpoints.restore_checkpoint(\n",
    "    ckpt_dir='my_checkpoints/',\n",
    "    target=model_state,\n",
    "    prefix='my_model',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df0b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.bind(trained_model_state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae164e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

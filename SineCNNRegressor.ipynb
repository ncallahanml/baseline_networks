{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3e9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "from wave_generator import WaveGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309db4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train_test_split(*tensors, split=.8):\n",
    "    n_samples = tensors[0].shape[0]\n",
    "    train_size = int(split * n_samples)\n",
    "    test_size = n_samples - train_size\n",
    "    rand_indices = torch.randperm(n_samples)\n",
    "    train_indices, test_indices = torch.split(rand_indices, [train_size, test_size])\n",
    "    assert train_indices.shape[0] == train_size, f'{train_indices.shape} != {train_size}'\n",
    "    assert test_indices.shape[0] == n_samples - train_size, f'{test_indices.shape} != {n_samples - train_size}'\n",
    "\n",
    "    data_tensors = list()\n",
    "    for tensor in tensors:\n",
    "        assert tensor.shape[0] == n_samples\n",
    "        train_data = tensor[train_indices]\n",
    "        test_data = tensor[test_indices]\n",
    "        data_tensors.extend((train_data, test_data))\n",
    "    return data_tensors\n",
    "\n",
    "def torch_data_from_dict(dataset_dict, separate=True):\n",
    "    tensors = list()\n",
    "    for label, arr in dataset_dict.items():\n",
    "        # convert this from arr to tensor after numpy change\n",
    "        dataset_samples = arr.shape[0]\n",
    "        input_tensor = torch.from_numpy(arr)\n",
    "        label_tensor = torch.full((dataset_samples, ), label)\n",
    "        display(input_tensor.shape)\n",
    "        display(label_tensor.shape)\n",
    "        if separate:\n",
    "            tensors.append((input_tensor, label_tensor))\n",
    "        else:\n",
    "            tensor = torch.cat([input_tensor, label_tensor], dim=1)\n",
    "            tensors.append(tensor)\n",
    "            \n",
    "    if separate:\n",
    "        input_tensors, label_tensors = zip(*tensors)\n",
    "        input_tensor = torch.cat(input_tensors, dim=0).unsqueeze(axis=1).double()\n",
    "        label_tensor = torch.cat(label_tensors, dim=0).double()\n",
    "        display(input_tensor.shape)\n",
    "        display(label_tensor.shape)\n",
    "        return input_tensor, label_tensor\n",
    "    else:\n",
    "        tensor = torch.cat(tensors, dim=0).double()\n",
    "        return tensor\n",
    "    \n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, input_tensor, label_tensor):\n",
    "        assert input_tensor.shape[0] == label_tensor.shape[0], f'{input_tensor.shape} != {label_tensor.shape[0]}'\n",
    "        self._data = input_tensor\n",
    "        self._label = label_tensor\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._data.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self._data[index]\n",
    "        label = self._label[index]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5935513",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_wave_gen = WaveGen(size=1_000).linear_phase().cos().amp(.5).t_noise(std=.01, dof=5)\n",
    "small_wave_gen = WaveGen(size=1_000).linear_phase().cos().amp(.05).t_noise(std=.01, dof=5)\n",
    "flat_wave_gen = WaveGen(size=1_000).linear_phase().cos().amp(.005).t_noise(std=.01, dof=5)\n",
    "\n",
    "dataset_samples = 10_000\n",
    "dataset_dict = {\n",
    "    1. : large_wave_gen.sample(dataset_samples).samples,\n",
    "    .5 : small_wave_gen.sample(dataset_samples).samples,\n",
    "    0. : flat_wave_gen.sample(dataset_samples).samples,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466936f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 1, 1000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30000])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data, labels = torch_data_from_dict(dataset_dict)\n",
    "train_data, test_data, train_labels, test_labels = torch_train_test_split(data, labels, split=.9)\n",
    "\n",
    "train_dataset = TorchDataset(train_data, train_labels)\n",
    "test_dataset = TorchDataset(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65ce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        out_channels=16, \n",
    "        kernel_size=2, \n",
    "        stride=4, \n",
    "        pool_kernel=4, \n",
    "        drop=.4, \n",
    "        dense_size=32\n",
    "    ):\n",
    "        super(SineCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size, stride=stride)\n",
    "        conv1_out_shape = (input_size - kernel_size) // stride + 1\n",
    "        self.activation = nn.LeakyReLU(negative_slope=.2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=pool_kernel)\n",
    "        pool_out_shape = (conv1_out_shape - pool_kernel) // pool_kernel + 1\n",
    "        self.fc1 = nn.Linear(pool_out_shape * out_channels, dense_size)\n",
    "        self.dout1 = nn.Dropout(p=drop)\n",
    "        self.fc2 = nn.Linear(dense_size, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.output(x).squeeze()\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba54b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        dense_size=256,\n",
    "        n_skips=2,\n",
    "        reduction=.5,\n",
    "        drop=.4, \n",
    "    ):\n",
    "        super(SineMLP, self).__init__()\n",
    "        self.fc_input = nn.Linear(input_size, dense_size)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=.2)\n",
    "        self.skips = list()\n",
    "        for _ in range(n_skips):\n",
    "            if int(dense_size * reduction ** 2) < 1:\n",
    "                warnings.warn('Too many skips applied, ignoring')\n",
    "                continue\n",
    "            skip, dense_size = self.skip_layer(dense_size, reduction=reduction, drop=drop)\n",
    "            self.skips.append(skip)\n",
    "        if dense_size != 1:\n",
    "            self.fc_output = nn.Linear(dense_size, 1)\n",
    "        self.output = nn.Sigmoid()\n",
    "        return\n",
    "        \n",
    "    def skip_layer(self, input_size, reduction=.5, drop=.4):\n",
    "        l1_size = int(input_size * reduction)\n",
    "        l2_size = int(l1_size * reduction)\n",
    "\n",
    "        skip_block = nn.Sequential(\n",
    "            nn.Linear(input_size, l1_size),\n",
    "            nn.BatchNorm2d(l1_size),\n",
    "            nn.LeakyReLU(negative_slope=.2, inplace=True),\n",
    "            nn.Dropout(p=drop),\n",
    "            nn.Linear(l1_size, l2_size),\n",
    "            nn.BatchNorm2d(l2_size),\n",
    "        )\n",
    "        return skip_block, l2_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_input(x)\n",
    "        x = self.activation(x)\n",
    "        for skip in self.skips:\n",
    "            x += skip(x)\n",
    "        x = self.fc_output(x)\n",
    "        x = self.output(x).squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8891a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    test_dataset, \n",
    "    n_epochs=1000,\n",
    "    batch_size=64,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.RMSprop,\n",
    "    early_stop_patience=0,\n",
    "    test_full=True,\n",
    "    print_=False,\n",
    "):\n",
    "    optimizer = optimizer(model.parameters())\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    if early_stop_patience:\n",
    "        patience = 0\n",
    "        model_buffer = None\n",
    "        loss_buffer = torch.tensor(float('inf'))\n",
    "\n",
    "    items = list()\n",
    "    for epoch in range(n_epochs):\n",
    "        p = print_ & True\n",
    "        for (train_data, train_labels), (test_data, test_labels) in zip(train_dataloader, test_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(train_data)\n",
    "            \n",
    "            train_loss = criterion(train_output, train_labels)       \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            test_output = model(test_data)\n",
    "            test_loss = criterion(test_output, test_labels)\n",
    "\n",
    "            if p and not epoch % 10:\n",
    "                print('Train Loss', train_loss.item())\n",
    "                print('Test Loss', test_loss.item())\n",
    "                p = False\n",
    "                \n",
    "        if test_full:\n",
    "            train_data, train_label = train_dataset[:]\n",
    "            test_data, test_label = test_dataset[:]\n",
    "            train_output = model(train_data)\n",
    "            train_loss = criterion(train_output, train_label)\n",
    "            test_output = model(test_data)\n",
    "            test_loss = criterion(test_output, test_label)\n",
    "            if early_stop_patience:\n",
    "                if test_loss > loss_buffer:\n",
    "                    patience += 1\n",
    "                    if patience >= early_stop_patience:\n",
    "                        items = items[:-early_stop_patience]\n",
    "                        model = model_buffer\n",
    "                        break\n",
    "                else:\n",
    "                    model_buffer = model\n",
    "                    loss_buffer = test_loss\n",
    "        elif early_stop_patience:\n",
    "            warnings.warn('Early Stopping Patience argument unused, full data evaluation at end of epochs is disabled. \\n Set test_full to True for early stopping.')\n",
    "        \n",
    "        items.append((train_loss.item(), test_loss.item()))\n",
    "    return model, items\n",
    "\n",
    "def plot_loss(items, title='', step=1):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    train_loss, test_loss = zip(*items)\n",
    "    colors = sns.color_palette('Spectral', 8)\n",
    "    sns.lineplot(train_loss[::step], dashes=False, color=colors[0], label='Train MSE')\n",
    "    sns.lineplot(test_loss[::step], dashes=False, color=colors[3], label='Test MSE')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d117679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Set 1 out of 72\n",
      "2|48|1|2|16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Set 2 out of 72\n",
      "2|48|1|2|20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     22\u001b[0m model \u001b[38;5;241m=\u001b[39m SineCNN(\n\u001b[0;32m     23\u001b[0m     train_dataset\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     24\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39mout_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     dense_size\u001b[38;5;241m=\u001b[39mdense_size,\n\u001b[0;32m     29\u001b[0m )\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     model, items \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_full\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(re)\n",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, test_dataset, n_epochs, batch_size, criterion, optimizer, early_stop_patience, test_full, print_)\u001b[0m\n\u001b[0;32m     32\u001b[0m test_output \u001b[38;5;241m=\u001b[39m model(test_data)\n\u001b[0;32m     33\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m criterion(test_output, test_labels)\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, train_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, test_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_channelss = [2,4]\n",
    "kernel_sizes = [48]\n",
    "strides = [1,2,4,8]\n",
    "pool_kernels = [2,4,8]\n",
    "dense_sizes = [16,20,24]\n",
    "\n",
    "total_i = np.prod((len(out_channelss), len(kernel_sizes), len(strides), len(pool_kernels), len(dense_sizes)))\n",
    "\n",
    "columns = ['out_channels','kernel_size','stride','pool_kernel','dense_size','train_loss','test_loss']\n",
    "rows = list()\n",
    "n_epochs = 30\n",
    "batch_size = 128\n",
    "for i, (out_channels, kernel_size, stride, pool_kernel, dense_size) in enumerate(itertools.product(out_channelss, kernel_sizes, strides, pool_kernels, dense_sizes)):\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    if stride > kernel_size or stride > pool_kernel:\n",
    "        continue\n",
    "        \n",
    "    print('Input Set', i + 1, 'out of', total_i, end='\\n')\n",
    "    print(out_channels, kernel_size, stride, pool_kernel, dense_size, sep='|')\n",
    "    \n",
    "    model = SineCNN(\n",
    "        train_dataset.shape[2] - 1,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        dense_size=dense_size,\n",
    "    ).double()\n",
    "    \n",
    "    try:\n",
    "        model, items = train(\n",
    "            model, \n",
    "            train_dataset, \n",
    "            test_dataset, \n",
    "            n_epochs=n_epochs, \n",
    "            batch_size=batch_size, \n",
    "            test_full=True,\n",
    "            early_stop_patience=5,\n",
    "        ) \n",
    "    except RuntimeError as re:\n",
    "        print(re)\n",
    "        continue\n",
    "        \n",
    "    train_loss, test_loss = zip(*items)\n",
    "    train_loss = np.mean(train_loss[2:])\n",
    "    test_loss = np.mean(test_loss[2:])\n",
    "    \n",
    "    rows.append((out_channels, kernel_size, stride, pool_kernel, dense_size, train_loss, test_loss))\n",
    "\n",
    "grid_df = pd.DataFrame(rows)\n",
    "assert grid_df.shape[1] == len(columns), f'{grid_df.shape[1]} != {len(columns)}'\n",
    "grid_df.columns = columns\n",
    "grid_df.to_csv(f'training_io_{pd.to_datetime(\"today\").strftime(\"%Y-%m-%d:%H\")}.csv')\n",
    "display(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc6c97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in ['out_channels','kernel_size','stride','pool_kernel','dense_size']:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(data=grid_df, x=col, y='test_loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_out_channels, best_kernel_size, best_stride, best_pool_kernel = grid_df.iloc[grid_df['test_loss'].argmin(),:4].to_numpy(dtype=np.int32)\n",
    "\n",
    "# display(best_out_channels, best_kernel_size, best_stride, best_pool_kernel)\n",
    "model = SineCNN(\n",
    "    train_dataset.shape[2] - 1,\n",
    "    out_channels=best_out_channels,\n",
    "    kernel_size=best_kernel_size,\n",
    "    stride=best_stride,\n",
    "    pool_kernel=best_pool_kernel,\n",
    ").double()\n",
    "\n",
    "model, items = train(model, train_dataset, test_dataset, n_epochs=100, test_full=True, early_stop_patience=30) \n",
    "plot_loss(\n",
    "    items,\n",
    "    title=''\n",
    ")\n",
    "torch.save(model, 'best_cnn_regressor.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

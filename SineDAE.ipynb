{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da3e9272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import scipy.stats as st\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from copy import deepcopy\n",
    "\n",
    "from wave_generator import WaveGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309db4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train_test_split(*tensors, split=.8):\n",
    "    n_samples = tensors[0].shape[0]\n",
    "    train_size = int(split * n_samples)\n",
    "    test_size = n_samples - train_size\n",
    "    rand_indices = torch.randperm(n_samples)\n",
    "    train_indices, test_indices = torch.split(rand_indices, [train_size, test_size])\n",
    "    assert train_indices.shape[0] == train_size, f'{train_indices.shape} != {train_size}'\n",
    "    assert test_indices.shape[0] == n_samples - train_size, f'{test_indices.shape} != {n_samples - train_size}'\n",
    "\n",
    "    data_tensors = list()\n",
    "    for tensor in tensors:\n",
    "        assert tensor.shape[0] == n_samples\n",
    "        train_data = tensor[train_indices]\n",
    "        test_data = tensor[test_indices]\n",
    "        data_tensors.extend((train_data, test_data))\n",
    "    return data_tensors\n",
    "\n",
    "def torch_data_from_noisy_generators(dataset_generators):\n",
    "    tensors = list()\n",
    "    for dataset_generator in dataset_generators:\n",
    "        # convert this from arr to tensor after numpy change\n",
    "        original_arr = dataset_generator.samples\n",
    "        denoised_arr = dataset_generator.wave\n",
    "        assert original_arr.shape == denoised_arr.shape, f'{original_arr.shape} != {denoised_arr.shape}'\n",
    "        original_tensor = torch.from_numpy(original_arr)\n",
    "        denoised_tensor = torch.from_numpy(denoised_arr)\n",
    "        tensors.append((original_tensor, denoised_tensor))\n",
    "            \n",
    "    original_tensors, denoised_tensors = zip(*tensors)\n",
    "    original_tensor = torch.cat(original_tensors, dim=0).unsqueeze(axis=1).double()\n",
    "    denoised_tensor = torch.cat(denoised_tensors, dim=0).double()\n",
    "    return original_tensor, denoised_tensor\n",
    "    \n",
    "class TorchDenoisingDataset(Dataset):\n",
    "    def __init__(self, input_tensor, output_tensor):\n",
    "        assert input_tensor.shape[0] == output_tensor.shape[0], f'{input_tensor.shape} != {output_tensor.shape[0]}'\n",
    "        self._original = input_tensor\n",
    "        self._denoised = output_tensor\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self._original.shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._original.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        original = self._original[index]\n",
    "        denoised = self._denoised[index]\n",
    "        return original, denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5935513",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_wave_gen = WaveGen(size=1_000).linear_phase().cos().amp(.5).t_noise(std=.01, dof=5)\n",
    "small_wave_gen = WaveGen(size=1_000).linear_phase().cos().amp(.05).t_noise(std=.01, dof=5)\n",
    "flat_wave_gen = WaveGen(size=1_000).linear_phase().cos().amp(.005).t_noise(std=.01, dof=5)\n",
    "\n",
    "dataset_samples = 10_000\n",
    "dataset_generators = [\n",
    "    large_wave_gen.sample(dataset_samples),\n",
    "    small_wave_gen.sample(dataset_samples),\n",
    "    flat_wave_gen.sample(dataset_samples),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "466936f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original, denoised = torch_data_from_noisy_generators(dataset_generators)\n",
    "train_original, test_original, train_denoised, test_denoised = torch_train_test_split(original, denoised, split=.8)\n",
    "\n",
    "train_dataset = TorchDenoisingDataset(train_original, train_denoised)\n",
    "test_dataset = TorchDenoisingDataset(test_original, test_denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1bfe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineDAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        out_channels=16, \n",
    "        kernel_size=2, \n",
    "        stride=4, \n",
    "        pool_kernel=4, \n",
    "        drop=.4, \n",
    "        alpha=.2,\n",
    "    ):\n",
    "        super(SineDAE, self).__init__()\n",
    "        conv1_shape = (input_size - kernel_size) // stride + 1\n",
    "        pool_shape = (conv1_shape - pool_kernel) // pool_kernel + 1\n",
    "        conv1t_shape = (out_channels - 1) * stride + (kernel_size - 1) + 1\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size, stride=stride),\n",
    "            nn.LeakyReLU(negative_slope=alpha),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(pool_shape, 1, kernel_size, stride=stride),\n",
    "            nn.LeakyReLU(negative_slope=alpha),\n",
    "            nn.Linear(conv1t_shape, input_size + 1),\n",
    "        )\n",
    "    \n",
    "        print(self.decoder[0])\n",
    "    \n",
    "        return\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "#         display(x_enc.shape)\n",
    "        x_enc = x_enc.transpose(1, 2)\n",
    "#         display(x_enc.shape)\n",
    "        x_dec = self.decoder(x_enc).squeeze(dim=1)\n",
    "        return x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8891a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    test_dataset, \n",
    "    n_epochs=1000,\n",
    "    batch_size=64,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.RMSprop,\n",
    "    early_stop_patience=0,\n",
    "    test_full=True,\n",
    "    print_=False,\n",
    "):\n",
    "    optimizer = optimizer(model.parameters())\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    if early_stop_patience:\n",
    "        patience = 0\n",
    "        model_buffer = None\n",
    "        loss_buffer = torch.tensor(float('inf'))\n",
    "\n",
    "    items = list()\n",
    "    for epoch in range(n_epochs):\n",
    "        p = print_ & True\n",
    "        for (train_original, train_denoised), (test_original, test_denoised) in zip(train_dataloader, test_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            train_output = model(train_original)\n",
    "            \n",
    "            assert train_output.shape == train_denoised.shape, f'{train_output.shape} != {train_denoised.shape}'\n",
    "            train_loss = criterion(train_output, train_denoised)       \n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            test_output = model(test_original)\n",
    "            test_loss = criterion(test_output, test_denoised)\n",
    "\n",
    "            if p and not epoch % 10:\n",
    "                print('Train Loss', train_loss.item())\n",
    "                print('Test Loss', test_loss.item())\n",
    "                p = False\n",
    "                \n",
    "        if test_full:\n",
    "            train_original, train_denoised = train_dataset[:]\n",
    "            test_original, test_denoised = test_dataset[:]\n",
    "            train_output = model(train_original)\n",
    "            train_loss = criterion(train_output, train_denoised)\n",
    "            test_output = model(test_original)\n",
    "            test_loss = criterion(test_output, test_denoised)\n",
    "            if early_stop_patience:\n",
    "                if test_loss > loss_buffer:\n",
    "                    patience += 1\n",
    "                    if patience >= early_stop_patience:\n",
    "                        items = items[:-early_stop_patience]\n",
    "                        model = model_buffer\n",
    "                        break\n",
    "                else:\n",
    "                    model_buffer = model\n",
    "                    loss_buffer = test_loss\n",
    "        elif early_stop_patience:\n",
    "            warnings.warn('Early Stopping Patience argument unused, full data evaluation at end of epochs is disabled. \\n Set test_full to True for early stopping.')\n",
    "        \n",
    "        items.append((train_loss.item(), test_loss.item()))\n",
    "    return model, items\n",
    "\n",
    "def plot_loss(items, title='', step=1):\n",
    "    plt.figure(figsize=(16,8))\n",
    "    train_loss, test_loss = zip(*items)\n",
    "    colors = sns.color_palette('Spectral', 8)\n",
    "    sns.lineplot(train_loss[::step], dashes=False, color=colors[0], label='Train MSE')\n",
    "    sns.lineplot(test_loss[::step], dashes=False, color=colors[3], label='Test MSE')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1422d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Set 1 out of 240\n",
      "2|16|1|2\n",
      "ConvTranspose1d(492, 1, kernel_size=(16,), stride=(1,))\n",
      "Input Set 2 out of 240\n",
      "2|16|1|4\n",
      "ConvTranspose1d(246, 1, kernel_size=(16,), stride=(1,))\n",
      "Input Set 3 out of 240\n",
      "2|16|1|8\n",
      "ConvTranspose1d(123, 1, kernel_size=(16,), stride=(1,))\n",
      "Input Set 4 out of 240\n",
      "2|16|1|16\n",
      "ConvTranspose1d(61, 1, kernel_size=(16,), stride=(1,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Nick\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Set 5 out of 240\n",
      "2|16|2|2\n",
      "ConvTranspose1d(246, 1, kernel_size=(16,), stride=(2,))\n",
      "Input Set 6 out of 240\n",
      "2|16|2|4\n",
      "ConvTranspose1d(123, 1, kernel_size=(16,), stride=(2,))\n",
      "Input Set 7 out of 240\n",
      "2|16|2|8\n",
      "ConvTranspose1d(61, 1, kernel_size=(16,), stride=(2,))\n",
      "Input Set 8 out of 240\n",
      "2|16|2|16\n",
      "ConvTranspose1d(30, 1, kernel_size=(16,), stride=(2,))\n",
      "Input Set 10 out of 240\n",
      "2|16|4|4\n",
      "ConvTranspose1d(61, 1, kernel_size=(16,), stride=(4,))\n",
      "Input Set 11 out of 240\n",
      "2|16|4|8\n",
      "ConvTranspose1d(30, 1, kernel_size=(16,), stride=(4,))\n",
      "Input Set 12 out of 240\n",
      "2|16|4|16\n",
      "ConvTranspose1d(15, 1, kernel_size=(16,), stride=(4,))\n",
      "Input Set 15 out of 240\n",
      "2|16|8|8\n",
      "ConvTranspose1d(15, 1, kernel_size=(16,), stride=(8,))\n",
      "Missing loss\n",
      "Input Set 16 out of 240\n",
      "2|16|8|16\n",
      "ConvTranspose1d(7, 1, kernel_size=(16,), stride=(8,))\n",
      "Input Set 20 out of 240\n",
      "2|16|16|16\n",
      "ConvTranspose1d(3, 1, kernel_size=(16,), stride=(16,))\n",
      "Input Set 21 out of 240\n",
      "2|32|1|2\n",
      "ConvTranspose1d(484, 1, kernel_size=(32,), stride=(1,))\n",
      "Input Set 22 out of 240\n",
      "2|32|1|4\n",
      "ConvTranspose1d(242, 1, kernel_size=(32,), stride=(1,))\n",
      "Input Set 23 out of 240\n",
      "2|32|1|8\n",
      "ConvTranspose1d(121, 1, kernel_size=(32,), stride=(1,))\n",
      "Input Set 24 out of 240\n",
      "2|32|1|16\n",
      "ConvTranspose1d(60, 1, kernel_size=(32,), stride=(1,))\n",
      "Input Set 25 out of 240\n",
      "2|32|2|2\n",
      "ConvTranspose1d(242, 1, kernel_size=(32,), stride=(2,))\n",
      "Input Set 26 out of 240\n",
      "2|32|2|4\n",
      "ConvTranspose1d(121, 1, kernel_size=(32,), stride=(2,))\n",
      "Input Set 27 out of 240\n",
      "2|32|2|8\n",
      "ConvTranspose1d(60, 1, kernel_size=(32,), stride=(2,))\n",
      "Input Set 28 out of 240\n",
      "2|32|2|16\n",
      "ConvTranspose1d(30, 1, kernel_size=(32,), stride=(2,))\n",
      "Input Set 30 out of 240\n",
      "2|32|4|4\n",
      "ConvTranspose1d(60, 1, kernel_size=(32,), stride=(4,))\n",
      "Input Set 31 out of 240\n",
      "2|32|4|8\n",
      "ConvTranspose1d(30, 1, kernel_size=(32,), stride=(4,))\n",
      "Input Set 32 out of 240\n",
      "2|32|4|16\n",
      "ConvTranspose1d(15, 1, kernel_size=(32,), stride=(4,))\n",
      "Missing loss\n",
      "Input Set 35 out of 240\n",
      "2|32|8|8\n",
      "ConvTranspose1d(15, 1, kernel_size=(32,), stride=(8,))\n",
      "Input Set 36 out of 240\n",
      "2|32|8|16\n",
      "ConvTranspose1d(7, 1, kernel_size=(32,), stride=(8,))\n",
      "Input Set 41 out of 240\n",
      "2|48|1|2\n",
      "ConvTranspose1d(476, 1, kernel_size=(48,), stride=(1,))\n",
      "Input Set 42 out of 240\n",
      "2|48|1|4\n",
      "ConvTranspose1d(238, 1, kernel_size=(48,), stride=(1,))\n",
      "Input Set 43 out of 240\n",
      "2|48|1|8\n",
      "ConvTranspose1d(119, 1, kernel_size=(48,), stride=(1,))\n",
      "Input Set 44 out of 240\n",
      "2|48|1|16\n",
      "ConvTranspose1d(59, 1, kernel_size=(48,), stride=(1,))\n"
     ]
    }
   ],
   "source": [
    "out_channelss = [2,4,8]\n",
    "kernel_sizes = [16,32,48,64]\n",
    "strides = [1,2,4,8,None]\n",
    "pool_kernels = [2,4,8,16]\n",
    "\n",
    "total_i = np.prod((len(out_channelss), len(kernel_sizes), len(strides), len(pool_kernels)))\n",
    "\n",
    "columns = ['out_channels','kernel_size','stride','pool_kernel','train_loss','test_loss']\n",
    "rows = list()\n",
    "n_epochs = 30\n",
    "batch_size = 128\n",
    "for i, (out_channels, kernel_size, stride, pool_kernel) in enumerate(itertools.product(out_channelss, kernel_sizes, strides, pool_kernels)):\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    if stride > kernel_size or stride > pool_kernel:\n",
    "        continue\n",
    "        \n",
    "    print('Input Set', i + 1, 'out of', total_i, end='\\n')\n",
    "    print(out_channels, kernel_size, stride, pool_kernel, sep='|')\n",
    "    \n",
    "    model = SineDAE(\n",
    "        train_dataset.shape[2] - 1,\n",
    "        out_channels=out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "    ).double()\n",
    "    \n",
    "    try:\n",
    "        model, items = train(\n",
    "            model, \n",
    "            train_dataset, \n",
    "            test_dataset, \n",
    "            n_epochs=n_epochs, \n",
    "            batch_size=batch_size, \n",
    "            test_full=True,\n",
    "            early_stop_patience=5,\n",
    "        )\n",
    "    except RuntimeError as re:\n",
    "        warnings.warn(re)\n",
    "        continue\n",
    "        \n",
    "    if not len(items):\n",
    "        print('Missing loss')\n",
    "        continue\n",
    "    train_loss, test_loss = zip(*items)\n",
    "    train_loss = np.mean(train_loss[2:])\n",
    "    test_loss = np.mean(test_loss[2:])\n",
    "    \n",
    "    rows.append((out_channels, kernel_size, stride, pool_kernel, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d117679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_df = pd.DataFrame(rows)\n",
    "assert grid_df.shape[1] == len(columns), f'{grid_df.shape[1]} != {len(columns)}'\n",
    "grid_df.columns = columns\n",
    "grid_df.to_csv(f'training_io_{pd.to_datetime(\"today\").strftime(\"%Y-%m-%d:%H\")}.csv')\n",
    "display(grid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc6c97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for col in ['out_channels','kernel_size','stride','pool_kernel']:\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.barplot(data=grid_df, x=col, y='test_loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_out_channels, best_kernel_size, best_stride, best_pool_kernel = grid_df.iloc[grid_df['test_loss'].argmin(),:4].to_numpy(dtype=np.int32)\n",
    "\n",
    "# display(best_out_channels, best_kernel_size, best_stride, best_pool_kernel)\n",
    "model = SineDAE(\n",
    "    train_dataset.shape[2] - 1,\n",
    "    out_channels=best_out_channels,\n",
    "    kernel_size=best_kernel_size,\n",
    "    stride=best_stride,\n",
    "    pool_kernel=best_pool_kernel,\n",
    ").double()\n",
    "\n",
    "model, items = train(model, train_dataset, test_dataset, n_epochs=100, test_full=True, early_stop_patience=30) \n",
    "plot_loss(\n",
    "    items,\n",
    "    title=''\n",
    ")\n",
    "torch.save(model, 'best_dae_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc07b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(test_dataset.shape[0], size=5):\n",
    "    x, y = test_dataset[i]\n",
    "    \n",
    "    z = model(x.unsqueeze(dim=1)).detach().numpy().squeeze()\n",
    "    x = x.squeeze()\n",
    "\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.plot(x, color='blue', label='Original')\n",
    "    plt.plot(y, color='red', label='Ground Truth')\n",
    "    plt.plot(z, color='purple', label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2f2c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

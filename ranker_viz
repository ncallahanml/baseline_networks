digraph {
	graph [size="35.699999999999996,35.699999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1701653345152 [label="
 (1, 5)" fillcolor=darkolivegreen1]
	1701651673920 [label=DivBackward0]
	1701651676704 -> 1701651673920
	1701651676704 [label=SubBackward0]
	1701651671472 -> 1701651676704
	1701651671472 [label=AddmmBackward0]
	1701651674880 -> 1701651671472
	1701653343472 [label="output_layer.bias
 (5)" fillcolor=lightblue]
	1701653343472 -> 1701651674880
	1701651674880 [label=AccumulateGrad]
	1701651674496 -> 1701651671472
	1701651674496 [label=CatBackward0]
	1701651674784 -> 1701651674496
	1701651674784 [label=AddmmBackward0]
	1701651673728 -> 1701651674784
	1701653342352 [label="partial_subnets.0.bias
 (1)" fillcolor=lightblue]
	1701653342352 -> 1701651673728
	1701651673728 [label=AccumulateGrad]
	1701651673296 -> 1701651674784
	1701651673296 [label=CatBackward0]
	1701651675936 -> 1701651673296
	1701651675936 [label=AddmmBackward0]
	1701651671568 -> 1701651675936
	1701653340192 [label="full_subnets.0.bias
 (1)" fillcolor=lightblue]
	1701653340192 -> 1701651671568
	1701651671568 [label=AccumulateGrad]
	1701651676176 -> 1701651675936
	1701651676176 [label=TBackward0]
	1701651673824 -> 1701651676176
	1701652716720 [label="full_subnets.0.weight
 (1, 5)" fillcolor=lightblue]
	1701652716720 -> 1701651673824
	1701651673824 [label=AccumulateGrad]
	1701651676272 -> 1701651673296
	1701651676272 [label=AddmmBackward0]
	1701651673776 -> 1701651676272
	1701653341472 [label="full_subnets.1.bias
 (1)" fillcolor=lightblue]
	1701653341472 -> 1701651673776
	1701651673776 [label=AccumulateGrad]
	1701651671760 -> 1701651676272
	1701651671760 [label=TBackward0]
	1701651673632 -> 1701651671760
	1701653340832 [label="full_subnets.1.weight
 (1, 5)" fillcolor=lightblue]
	1701653340832 -> 1701651673632
	1701651673632 [label=AccumulateGrad]
	1701651674160 -> 1701651673296
	1701651674160 [label=AddmmBackward0]
	1701651671184 -> 1701651674160
	1701653341552 [label="full_subnets.2.bias
 (1)" fillcolor=lightblue]
	1701653341552 -> 1701651671184
	1701651671184 [label=AccumulateGrad]
	1701651671616 -> 1701651674160
	1701651671616 [label=TBackward0]
	1701651671232 -> 1701651671616
	1701653342272 [label="full_subnets.2.weight
 (1, 5)" fillcolor=lightblue]
	1701653342272 -> 1701651671232
	1701651671232 [label=AccumulateGrad]
	1701651674112 -> 1701651673296
	1701651674112 [label=AddmmBackward0]
	1701651672240 -> 1701651674112
	1701653342192 [label="full_subnets.3.bias
 (1)" fillcolor=lightblue]
	1701653342192 -> 1701651672240
	1701651672240 [label=AccumulateGrad]
	1701651671952 -> 1701651674112
	1701651671952 [label=TBackward0]
	1701651672192 -> 1701651671952
	1701653342032 [label="full_subnets.3.weight
 (1, 5)" fillcolor=lightblue]
	1701653342032 -> 1701651672192
	1701651672192 [label=AccumulateGrad]
	1701651673152 -> 1701651674784
	1701651673152 [label=TBackward0]
	1701651672000 -> 1701651673152
	1701653343232 [label="partial_subnets.0.weight
 (1, 4)" fillcolor=lightblue]
	1701653343232 -> 1701651672000
	1701651672000 [label=AccumulateGrad]
	1701651676800 -> 1701651674496
	1701651676800 [label=AddmmBackward0]
	1701651672288 -> 1701651676800
	1701653342672 [label="partial_subnets.1.bias
 (1)" fillcolor=lightblue]
	1701653342672 -> 1701651672288
	1701651672288 [label=AccumulateGrad]
	1701651676032 -> 1701651676800
	1701651676032 [label=CatBackward0]
	1701651675936 -> 1701651676032
	1701651676320 -> 1701651676032
	1701651676320 [label=AddmmBackward0]
	1701651671376 -> 1701651676320
	1701653340672 [label="full_subnets.4.bias
 (1)" fillcolor=lightblue]
	1701653340672 -> 1701651671376
	1701651671376 [label=AccumulateGrad]
	1701651674832 -> 1701651676320
	1701651674832 [label=TBackward0]
	1701651671280 -> 1701651674832
	1701653340752 [label="full_subnets.4.weight
 (1, 5)" fillcolor=lightblue]
	1701653340752 -> 1701651671280
	1701651671280 [label=AccumulateGrad]
	1701651672144 -> 1701651676032
	1701651672144 [label=AddmmBackward0]
	1701651674400 -> 1701651672144
	1701653341232 [label="full_subnets.5.bias
 (1)" fillcolor=lightblue]
	1701653341232 -> 1701651674400
	1701651674400 [label=AccumulateGrad]
	1701651673056 -> 1701651672144
	1701651673056 [label=TBackward0]
	1701651674208 -> 1701651673056
	1701653341632 [label="full_subnets.5.weight
 (1, 5)" fillcolor=lightblue]
	1701653341632 -> 1701651674208
	1701651674208 [label=AccumulateGrad]
	1701651671520 -> 1701651676032
	1701651671520 [label=AddmmBackward0]
	1701651671328 -> 1701651671520
	1701653341392 [label="full_subnets.6.bias
 (1)" fillcolor=lightblue]
	1701653341392 -> 1701651671328
	1701651671328 [label=AccumulateGrad]
	1701651672576 -> 1701651671520
	1701651672576 [label=TBackward0]
	1701651674064 -> 1701651672576
	1701653342752 [label="full_subnets.6.weight
 (1, 5)" fillcolor=lightblue]
	1701653342752 -> 1701651674064
	1701651674064 [label=AccumulateGrad]
	1701651675888 -> 1701651676800
	1701651675888 [label=TBackward0]
	1701651676848 -> 1701651675888
	1701651708272 [label="partial_subnets.1.weight
 (1, 4)" fillcolor=lightblue]
	1701651708272 -> 1701651676848
	1701651676848 [label=AccumulateGrad]
	1701651675840 -> 1701651674496
	1701651675840 [label=AddmmBackward0]
	1701651673248 -> 1701651675840
	1701653342912 [label="partial_subnets.2.bias
 (1)" fillcolor=lightblue]
	1701653342912 -> 1701651673248
	1701651673248 [label=AccumulateGrad]
	1701651671712 -> 1701651675840
	1701651671712 [label=CatBackward0]
	1701651676272 -> 1701651671712
	1701651676320 -> 1701651671712
	1701651675360 -> 1701651671712
	1701651675360 [label=AddmmBackward0]
	1701651672096 -> 1701651675360
	1701653342592 [label="full_subnets.7.bias
 (1)" fillcolor=lightblue]
	1701653342592 -> 1701651672096
	1701651672096 [label=AccumulateGrad]
	1701651672480 -> 1701651675360
	1701651672480 [label=TBackward0]
	1701589962608 -> 1701651672480
	1701653342512 [label="full_subnets.7.weight
 (1, 5)" fillcolor=lightblue]
	1701653342512 -> 1701589962608
	1701589962608 [label=AccumulateGrad]
	1701651675744 -> 1701651671712
	1701651675744 [label=AddmmBackward0]
	1702483047664 -> 1701651675744
	1701653342112 [label="full_subnets.8.bias
 (1)" fillcolor=lightblue]
	1701653342112 -> 1702483047664
	1702483047664 [label=AccumulateGrad]
	1701651673968 -> 1701651675744
	1701651673968 [label=TBackward0]
	1701650382080 -> 1701651673968
	1701653341952 [label="full_subnets.8.weight
 (1, 5)" fillcolor=lightblue]
	1701653341952 -> 1701650382080
	1701650382080 [label=AccumulateGrad]
	1701651672336 -> 1701651675840
	1701651672336 [label=TBackward0]
	1701651676896 -> 1701651672336
	1701653342832 [label="partial_subnets.2.weight
 (1, 4)" fillcolor=lightblue]
	1701653342832 -> 1701651676896
	1701651676896 [label=AccumulateGrad]
	1701651676128 -> 1701651674496
	1701651676128 [label=AddmmBackward0]
	1701651676656 -> 1701651676128
	1701653344192 [label="partial_subnets.3.bias
 (1)" fillcolor=lightblue]
	1701653344192 -> 1701651676656
	1701651676656 [label=AccumulateGrad]
	1701650374352 -> 1701651676128
	1701650374352 [label=CatBackward0]
	1701651674160 -> 1701650374352
	1701651672144 -> 1701650374352
	1701651675360 -> 1701650374352
	1701650374256 -> 1701650374352
	1701650374256 [label=AddmmBackward0]
	1701650374304 -> 1701650374256
	1701653342432 [label="full_subnets.9.bias
 (1)" fillcolor=lightblue]
	1701653342432 -> 1701650374304
	1701650374304 [label=AccumulateGrad]
	1701650374160 -> 1701650374256
	1701650374160 [label=TBackward0]
	1701650367872 -> 1701650374160
	1701653343152 [label="full_subnets.9.weight
 (1, 5)" fillcolor=lightblue]
	1701653343152 -> 1701650367872
	1701650367872 [label=AccumulateGrad]
	1701650381696 -> 1701651676128
	1701650381696 [label=TBackward0]
	1701650374736 -> 1701650381696
	1701653343712 [label="partial_subnets.3.weight
 (1, 4)" fillcolor=lightblue]
	1701653343712 -> 1701650374736
	1701650374736 [label=AccumulateGrad]
	1701651673536 -> 1701651674496
	1701651673536 [label=AddmmBackward0]
	1701650374496 -> 1701651673536
	1701653344112 [label="partial_subnets.4.bias
 (1)" fillcolor=lightblue]
	1701653344112 -> 1701650374496
	1701650374496 [label=AccumulateGrad]
	1701650368064 -> 1701651673536
	1701650368064 [label=CatBackward0]
	1701651674112 -> 1701650368064
	1701651671520 -> 1701650368064
	1701651675744 -> 1701650368064
	1701650374256 -> 1701650368064
	1701650367968 -> 1701651673536
	1701650367968 [label=TBackward0]
	1701650381984 -> 1701650367968
	1701653344032 [label="partial_subnets.4.weight
 (1, 4)" fillcolor=lightblue]
	1701653344032 -> 1701650381984
	1701650381984 [label=AccumulateGrad]
	1701651674688 -> 1701651671472
	1701651674688 [label=TBackward0]
	1701651673584 -> 1701651674688
	1701653343792 [label="output_layer.weight
 (5, 5)" fillcolor=lightblue]
	1701653343792 -> 1701651673584
	1701651673584 [label=AccumulateGrad]
	1701651672672 -> 1701651676704
	1701651672672 [label=MinBackward1]
	1701651671472 -> 1701651672672
	1701651674544 -> 1701651673920
	1701651674544 [label=MaxBackward1]
	1701651676704 -> 1701651674544
	1701651673920 -> 1701653345152
	1701653448320 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	1701651673200 [label=CatBackward0]
	1701651675936 -> 1701651673200
	1701651676272 -> 1701651673200
	1701651674160 -> 1701651673200
	1701651674112 -> 1701651673200
	1701651676320 -> 1701651673200
	1701651672144 -> 1701651673200
	1701651671520 -> 1701651673200
	1701651675360 -> 1701651673200
	1701651675744 -> 1701651673200
	1701650374256 -> 1701651673200
	1701651673200 -> 1701653448320
}
